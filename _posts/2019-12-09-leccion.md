---
layout: post
title:  "Clase del Lunes 09/12/2019"
categories: Clases
code: https://github.com/ULL-MII-CA-1819/nodejs-the-right-way/blob/master/transforming-data-and-testing-continuously-chapter-5/databases
---



# Clase del Lunes 09/12/2019

## Anuncio

Incidencia en el servicio IaaS y sistemas de cómputo. Contacto: http://soporte.ull.es/stic (soporte@ull.es). 
Máquinas apagadas.

## Transforming Data and Testing Continuously. Chapter 5 of the Book Node.js the Right Way

Este es el primer capítulo de una serie que cubre los capítulos del 5 al 9 en el que se construye una aplicación Web.

En este capítulos 5 vamos a obtener los datos con los que vamos a cargar nuestra base de datos para la web app:

* Descargar desde el Proyecto Gutenberg (ebooks gratuitos) el catálogo de libros en [Resource Description Framework RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework). 

  [![Guttenberg project](/assets/images/guttenberg.png)](http://www.gutenberg.org/)

El RDF es una familia de especificaciones de la World Wide Web Consortium (W3C) para describir en XML los datos que aparecen en un recurso web.

We will be treating the RDF files like regular, undifferentiated XML files for parsing and for data extraction.

* [RDF](https://es.wikipedia.org/wiki/Resource_Description_Framework)
    - El Marco de Descripción de Recursos (del inglés Resource Description Framework, RDF) es una familia de especificaciones de la World Wide Web Consortium (W3C) originalmente diseñado como un modelo de datos para metadatos
* [JavaScript Object Notation for Linked Data, JSON_LD](https://en.wikipedia.org/wiki/JSON-LD)


* Extraeremos del fichero RDF/XML la información que nos interesa, produciendo como salida un fichero JSON con la misma
   - La idea es escribir algo así como un programa `rdf-to-json.js` que funcione así:
   ```
   ~/.../transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ./rdf-to-json.js ../data/cache/epub/12/pg12.rdf 
   ```

  - Que deberá producir una  salida parecida a esta:

   ```json
        {
        "id": 12,
        "title": "Through the Looking-Glass",
        "authors": [
            "Carroll, Lewis"
        ],
        "subjects": [
            "Fantasy literature"
        ]
        }
    ```
* Estos JSON van a guardarse en una base de Datos denominada Elastic Search que va a ser usada en el capítulo 6 del libro (Commanding Databases). 
  - Elasticsearch es una base de datos NoSQL que funciona como un servicio REST a la que se accede vía HTTP  y que almacena e indexa documentos JSON. En el capítulo 6 se construye un programa de línea de comandos que permite interactuar con la base de datos que se crea en el capítulo 5.
 - Este es el esquema de trabajo para los dos capítulos:
  - ![xml 2 json and json 2 elastic search](/assets/images/ch5-xml-2-json-ch6-2-es.png)


## Line Delimited JSON

No vamos a usar JSON como formato de salida, sino Line Delimited JSON.

[Line Delimited JSON (JDL)](https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON), newline-delimited **JSON** (**NDJSON**), and **JSON lines** (**JSONL**) are three terms for equivalent formats of JSON streaming.

Streaming makes use of the fact that the JSON format does not allow newline and return characters within primitive values (in strings those must be escaped as `\n` and `\r`, respectively) and that most JSON formatters default to not including any whitespace, including newlines and returns.
These features allow the <u>newline</u> and/or return characters to be used as a delimiter.



This format is documented at the [JSON Lines website](http://jsonlines.org/).

This example shows two JSON objects (the implicit newline characters at the end of each line are not shown):

```json
{"some":"thing\n"}
```

```json
{"may":{"include":"nested","objects":["and","arrays"]}}
```

The use of a newline as a delimiter enables this format to work very well with [traditional line-oriented Unix tools](https://en.wikipedia.org/wiki/Pipeline_(Unix)).


## Estructura de directorios

Este es un ejemplo de como estructurar este proyecto:

⇐
LEFTWARDS DOUBLE ARROW
Unicode: U+21D0, UTF-8: E2 87 90

```
$ tree -s  -I 'node_modules*|epub*|jim*|images'
.
├── [        192]  data
│   ├── [        294]  README.md
│   ├── [   13005066]  bulk_pg.ldj       ⇐ Fichero en formato LDJ con todo el catálogo
│   ├── [   10809492]  bulk_result.json  ⇐ Lo generaremos en el capítulo 6
│   └── [         96]  cache             ⇐ Donde quedarán los ficheros RDF de la descarga
└── [        256]  databases
    ├── [      38863]  README.md
    ├── [         96]  lib
    │   └── [       3665]  parse-rdf.js
    ├── [        786]  rdf-to-bulk.js
    ├── [        338]  rdf-to-json.js
    └── [        128]  test
        ├── [       1101]  parse-rdf-test.js
        └── [      12393]  pg132.rdf

5 directories, 9 files
```

En el directorio `data` guardaremos los datos  descargados de Guttenberg y en el directorio `databases` nuestro código: ejecutables, librerías y las pruebas.

## Descarga de los datos

En esta página  encontrará el catálogo completo de Guttenberg en formato RDF/XML:

[![](/assets/images/guttenberg-catalog.png)](https://www.gutenberg.org/wiki/Gutenberg:Feeds#The_Complete_Project_Gutenberg_Catalog)

Para descargarlo nos  posicionamos en el directorio adecuado y podemos usar `curl`. El fichero está compactado:

```
~/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5(master)]$ sed -ne '/c5-get/,/^$/p'  ~/sol-nodejs-the-right-way/gulpfile.js 
```

```js
gulp.task("c5-get-guttenberg", shell.task(
  
    'cd transforming-data-and-testing-continuously-chapter-5/data && ' +
    'curl -O https://www.gutenberg.org/cache/epub/feeds/rdf-files.tar.bz2 &&' +
    'tar -xvjf rdf-files.tar.bz2'
));
```

* `curl option -O, --remote-name`
  -  Write  output to a local file named like the remote file we get. (Only the file part of the remote file is used, the path is cut off.) The file will be saved in the current working directory. If you want the file saved in a different directory,  make  sure        you change the current working directory before invoking curl with this option.  The remote file name to use for saving is extracted from the given URL, nothing else, and if it already exists it will be overwritten.
*  `tar -xvjf rdf-files.tar.bz2`
  -  `-x`      Extract to disk from the archive.  If a file with the same name appears more than once in the archive, each copy will be extracted, with later copies overwriting (replacing) earlier copies.
  -  `-j`  (c mode only) Compress the resulting archive with bzip2(1).  In extract or list modes, this option is ignored.  Note that unlike other tar implementations, this implementation recognizes bzip2 compression automatically when reading archives.
  -  `-f file`  Read the archive from or write the archive to the specified file.  The filename can be for standard input or standard output.
  -   `-v`  Produce verbose output.  In create and extract modes, tar will list each file name as it is read from or written to the archive.  In list mode, tar will produce output similar to that of ls(1).  Additional -v options will provide additional detail.

Cuando ejecutamos esta secuencia de comandos crearemos en el directorio `data` los ficheros `*.rdf`, una por libro:

```
x cache/epub/0/pg0.rdf
x cache/epub/1/pg1.rdf
x cache/epub/10/pg10.rdf
...
x cache/epub/9998/pg9998.rdf
x cache/epub/9999/pg9999.rdf
x cache/epub/999999/pg999999.rdf
...
```

Esto nos deja en `cache/epub/` un montón de directorios numerados:

```
$ ls data/cache/epub/ | head -n 4
0
1
10
100
$ ls data/cache/epub/ | tail -n 4
9999
999999
DELETE-52276
DELETE-55495
```

## Analizando la estructura de los ficheros RDF

Por ejemplo, aquí están los contenidos de [data/cache/epub/132/pg132.rdf](/tema3-web/pg132)

The important pieces of information that we’d like to extract are as follows:

* The Gutenberg ID (132)
* The book’s title
* The list of authors (agents) 
* The list of subjects

```
$  ./rdf-to-json.js ../data/cache/epub/132/pg132.rdf
```

```json 
{
  "id": 132,
  "title": "The Art of War",
  "authors": [
    "Sunzi, active 6th century B.C.",
    "Giles, Lionel"
  ],
  "subjects": [
    "Military art and science -- Early works to 1800",
    "War -- Early works to 1800"
  ]
}
```

## Las Pruebas


### Mocha y Chai

```
$ cd databases
$ npm init -y
$ npm install --save-dev --save-exact mocha chai
```

Chai supports several different styles of assertions, each with their own pros and cons.
Here We’ll use Chai’s expect style.

Here is an example, first using Node.js’s built-in assert module, 

```js
​   assert.ok(book.authors, ​'book should have authors'​);
​   assert.ok(Array.isArray(book.authors), ​'authors should be an array'​);
​   assert.equal(book.authors.length, 2, ​'authors length should be 2'​);
```

If you read the code carefully, you can deconstruct that it’s confirming that the `book` object
has a property called `authors` that is an `Array` of length 2. 

By contrast, check out this example using Chai’s expect method:

... and the same using Chai’s expect style:

```js
​   expect(book).to.have.a.property(​'authors'​)
​   .that.is.an(​'array'​).​with​.lengthOf(2);
```

By comparison to the native Node.js assert code, this reads like English. 

### Setting Up Tests with Mocha and Chai

```bash
$ ​​cd​​ ​​databases​
​$ ​​npm​​ ​​init​​ ​​-y​
$ ​​npm​​ ​​install​​ ​​--save-dev​​ ​​--save-exact​​ ​​mocha​​ ​​chai
```

### Ejecución de las Pruebas

```
[~/sol-nodejs-the-right-way(master)]$ jq .scripts package.json 
```

```json
{
  "test-databases": "mocha transforming-data-and-testing-continuously-chapter-5/databases/test/parse-rdf-test.js",
  "test-debug-databases": "mocha --inspect-brk --no-timeouts transforming-data-and-testing-continuously-chapter-5/databases/test/parse-rdf-test.js",
  "test:watch-databases": "mocha --watch --reporter min transforming-data-and-testing-continuously-chapter-5/databases/test/parse-rdf-test.js",
}
```


### BDD, TDD and The Red-Green-Refactor cycle


1. Add new criteria to the test.
2. Run the test and see that it fails.
3. Modify the code being tested.
4. Run the test and see that it passes.

[![https://files.realpython.com/media/tdd.0904607f8ec9.png](https://files.realpython.com/media/tdd.0904607f8ec9.png)](https://medium.com/@sukorenomw/why-tdd-test-driven-development-a1bc983a2cc0)

We keep this cycle going over and over again:

1. We write the test before a single line of implementation code is written.
    * That test will serve as a guideline what to build and to make sure we don’t break anything in the future that prevents the regular flow from working.
    * We describe each unit of the system through a failing test
2. We write the code to make it pass
3. Then, we can refactor it if necessary

### Código de las Pruebas

```
cp ../data/cache/epub/132/pg132.rdf test/
[~/.../Capitulo5/databases(master)]$ tree test/
test/
├── parse-rdf-test.js
└── pg132.rdf
```

```
$ cat databases/test/parse-rdf-test.js 
```

```js
'use strict';

const fs = require('fs');
const expect = require('chai').expect;
const parseRDF = require('../lib/parse-rdf.js');
const rdf = fs.readFileSync(`${__dirname}/pg132.rdf`);

describe("Chapter 5: Transforming Data and Testing Continuously", () => {
  describe('parseRDF', () => {
    it('should be a function', () => {
      debugger;
      expect(parseRDF).to.be.a('function');
    });
    it('should parse RDF content', () => {
      const book = parseRDF(rdf);
      // https://www.chaijs.com/api/bdd/#method_language-chains
      expect(book).to.be.a('object');

      expect(book).to.have.a.property('id', 132);

      expect(book).to.have.a.property('title', 'The Art of War');

      expect(book).to.have.a.property('authors').that.is.an('array').with.lengthOf(2).
      and.contains('Sunzi, active 6th century B.C.').
      and.contains('Giles, Lionel');

      expect(book).to.have.a.property('subjects').that.is.an('array').with.lengthOf(2).
      and.contains('Military art and science -- Early works to 1800').
      and.contains('War -- Early works to 1800');
    });
  });
});
```

### Depurando las Pruebas


Mocha options for debugging. The options:

```
--debug, --inspect, --debug-brk, --inspect-brk, debug, inspect
```

Enables Node.js' debugger or inspector.

Use `--inspect / --inspect-brk / --debug / --debug-brk`  to launch the V8 inspector for use with Chrome Dev Tools.

Use `inspect / debug` to launch Node.js' internal debugger.

All of these options are mutually exclusive.

Implies `--no-timeout`.

```
[~/.../p9-t3-transforming-data/transforming-data-and-testing-continuously-chapter-5(master)]$ npm run test-debug-databases

> nodejs-8-the-right-way@1.0.0 test-debug-databases /Users/casiano/local/src/CA/sol-nodejs-the-right-way
> mocha --inspect-brk --no-timeouts transforming-data-and-testing-continuously-chapter-5/databases/test/parse-rdf-test.js

Debugger listening on ws://127.0.0.1:9229/db1b1cee-1632-4541-9ef9-608b2943c3a3
For help, see: https://nodejs.org/en/docs/inspector
Debugger attached.
```

![/assets/images/chrome-debug-mocha.png](/assets/images/chrome-debug-mocha.png)

## Extracting Data from XML with Cheerio

Cheerio is a fast Node.js module that provides a jQuery-like API for working with HTML and XML documents.

* [cheerio](https://github.com/cheeriojs/cheerio) GitHub
* [https://cheerio.js.org/](https://cheerio.js.org/)

Cheerio uses CSS selectors as a Domain specific Language (DSL) to traverse and modify the Abstract Syntax Tree (AST) of te XML document. 
The AST of a HTML or XML document is usually referred as the DOM.

Other popular alternatives include 

* [jsdom](https://github.com/jsdom/jsdom)
* [xmldom](https://github.com/jindw/xmldom)

both of which are based on the W3C’s DOM specification.

In your own projects, if the XML files that you’re working with are quite large, then you’re probably going to want a streaming SAX  (parser instead. SAX, which stands for **Simple API for XML**, treats XML as a stream of tokens that your program digests in sequence. Unlike a DOM parser, which considers the document as a whole, a SAX parser operates on only a small piece at a time.

Compared to DOM parsers, SAX parsers can be quite fast and memory-efficient. But the downside of using a SAX parser is that your program will have to keep track of the document structure in flight 

* [sax-js](https://github.com/isaacs/sax-js)

### Getting Started with Cheerio

To get started with Cheerio, install it with npm and save the dependency.

```
​   ​$ ​​npm​​ ​​install​​ ​​--save​​ ​​--save-exact​​ ​​cheerio
```

Before we start using Cheerio, let’s create some BDD tests that we can make pass by doing so.

```sh
​   ​$ ​​npm​​ ​​run​​ ​​test:watch​
```

It should clear the screen and report two passing tests:

```
​   2 passing (44ms)
```

Now let’s require that the book object returned by `parseRDF` has the correct numeric `ID` for *The Art of War*. 

Open your `parse-rdf-test.js` file and expand the second test by adding a check that the book object has an `id` property containing the number 132.

[test/parse-rdf-test.js]({{page.code}}/test/parse-rdf-test.js)

```js
​  it(​'should parse RDF content'​, () => {
  ​   const​ book = parseRDF(rdf);
  ​   expect(book).to.be.an(​'object'​);
  ​   expect(book).to.have.a.property(​'id'​, 132);
​   });
```

This code takes advantage of Chai’s sentence-like BDD API

Since we have not yet implemented the code to include the ‘id‘ in the returned ‘book‘ object, as soon as you save the file, your Mocha terminal should report this:

```
1 passing (4ms)
1 failing
​
1) parseRDF should parse RDF content:
   AssertionError: expected {} to have a property 'id'
​   at Context.it (test/parse-rdf-test.js:32:28)
```

The test is failing exactly as we expect it should.

Now it’s time to use [Cheerio](https://github.com/cheeriojs/cheerio) to pull out the four fields we want:

1. the book’s ID, 
2. the title, 
3. the authors, and 
4. the subjects.

### Reading Data from an Attribute

The first piece of information we hope to extract using [Cheerio](https://github.com/cheeriojs/cheerio)  
is the book’s ID. 
Recall that we’re trying to grab the number 132 out of this XML tag:

**File [pg132.rdf](/tema3-web/pg132)**:

```xml
​   <pgterms:ebook rdf:about=​"ebooks/132"​>
```

Open your `lib/parse-rdf.js` file and make it look like the following:

[lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js)

```js
​'use strict'​;
​const​ cheerio = require(​'cheerio'​);

module.exports = rdf => {
​  const​ $ = cheerio.load(rdf);
​
​  const​ book = {};

  book.id = +$(​'pgterms​​\\​​:ebook'​).attr(​'rdf:about'​).replace(​'ebooks/'​, ​''​);

 ​return​ book;
​};
```

In CSS, the colon character (`:`) has special meaning—it is used to introduce pseudo selectors like `:hover` for links that are hovered over.

In our case, we need a literal colon character for the `<pgterms:ebook>` tag name, so we have to escape it with a backslash.

But since the backslash is a special character in JavaScript string literals, that too needs to be escaped-. Thus, our query selector for finding the tag is `pgterms\\:ebook`.

Lo ilustramos en esta sesión de node:

```js
> s = 'pgterms\:ebook\n'
'pgterms:ebook\n'
> console.log(`-${s}-`)
-pgterms:ebook
-
> t = 'pgterms\\:ebook\n'
'pgterms\\:ebook\n'
> console.log(`-${t}-`)
-pgterms\:ebook
```

### Reading the Text of a Node

Next, let’s add a test for the `title` of the book. 
Insert the following code:

[test/parse-rdf-test.js]({{page.code}}/databases/test/parse-rdf-test.js)

```js
​   expect(book).to.have.a.property(​'title'​, ​'The Art of War'​);
```

Your continuous testing terminal should read as follows:

```
   1 passing (3ms)
   1 failing

​   1) parseRDF should parse RDF content:
 ​     AssertionError: expected { id: 132 } to have a property 'title'
​      at Context.it (test/parse-rdf-test.js:35:28)
```

Now let’s grab the title and add it to the returned book object. Recall that the [XML](/tema3-web/pg132) containing the title looks like this:

```xml
​   <dcterms:title>The Art of War</dcterms:title>
```

Add the following to your [lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js) file, after the line where we set book.id:

[lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js)

```js
​   book.title = $(​'dcterms​​\\​​:title'​).text();
```

Using Cheerio, we select the tag named `dcterms:title` and used the `text()`method of the element to save its text contents to the `book.title` property.

### Collecting an Array of Values

Now, let’s tests the array of book authors:

[test/parse-rdf-test.js]({{page.code}}/test/parse-rdf-test.js)

```js
​  expect(book).to.have.a.property(​'authors'​)
​   that.is.an(​'array'​).​with​.lengthOf(2)
​  .and.contains(​'Sunzi, active 6th century B.C.'​)
​  .and.contains(​'Giles, Lionel'​);
```

In Chai’s language-chaining DSL, words like 

- `and`, 
- `that`, and 
- `which` 

are largely interchangeable. 
This let us write clauses like 

`.and.contains(’X’)` or `.that.contains(’X’)`, 

depending on which version reads better in your test case.

Our continuous testing reports a failure:

```
​   1 passing (11ms)
​   1 failing
​   
​   1) parseRDF should parse RDF content:
​   AssertionError: expected { id: 132, title: 'The Art of War' } to have a
​   property 'authors' at Context.it (test/parse-rdf-test.js:39:28)
```

To make the test pass, recall that we will need to pull out the content from these tags:

**[data/cache/epub/132/pg132.rdf](/tema3-web/pg132)**

```xml
​   <pgterms:agent rdf:about=​"2009/agents/4349"​>
  ​   <pgterms:name>Sunzi, active 6th century B.C.</pgterms:name>
​   </pgterms:agent>
​   <pgterms:agent rdf:about=​"2009/agents/5101"​>
  ​   <pgterms:name>Giles, Lionel</pgterms:name>
​   </pgterms:agent>
```

We’re looking to extract the text of each `<pgterms:name>` tag that’s a child of a `<pgterms:agent>`. 

The CSS selector `pgterms:agent pgterms:name` finds the elements we need, so we can start with this:

```js
​   $(​'pgterms​​\\​​:agent pgterms​​\\​​:name'​)
```

You might be tempted to grab the text straight away like this:

```js
​   book.authors = $(​'pgterms​​\\​​:agent pgterms​​\\​​:name'​).text();
```

But unfortunately, this won’t give us what we want, because Cheerio’s `text` method returns a single string and we need an array of strings. 

Instead, we use a `map`:

[lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js)

```js
book.authors = $(​'pgterms​​\\​​:agent pgterms​​\\​​:name'​)
  .toArray().map(elem => $(elem).text());
```

Calling Cheerio’s `.toArray` method converts the collection object into a true JavaScript `Array`. 

Unfortunately, the collection of objects that comes out of `toArray` doesn’t consist of Cheerio-wrapped objects, but rather `document` nodes. 

To extract the text using Cheerio’s `text`, we need to wrap each node with the `$` function, then call text on it. 

```js
 elem => $(elem).text().
 ```

### Traversing the Document: The List of Subjects

Finally, we want to obtain the list of subjects.

**File: [pg132.rdf](/tema3-web/pg132)**

```xml
​   <dcterms:subject>
  ​   <rdf:Description rdf:nodeID=​"N26bb21da0c924e5abcd5809a47f231e7"​>
    ​   <dcam:memberOf rdf:resource=​"http://purl.org/dc/terms/LCSH"​/>
    ​   <rdf:value>Military art and science -- Early works to 1800</rdf:value>
  ​   </rdf:Description>
​   </dcterms:subject>
​   <dcterms:subject>
  ​   <rdf:Description rdf:nodeID=​"N269948d6ecf64b6caf1c15139afd375b"​>
    ​   <rdf:value>War -- Early works to 1800</rdf:value>
    ​   <dcam:memberOf rdf:resource=​"http://purl.org/dc/terms/LCSH"​/>
  ​   </rdf:Description>
​   </dcterms:subject>
``` 

As with previous examples, let’s start by adding a test:

[test/parse-rdf-test.js]({{page.code}}/test/parse-rdf-test.js)

```js
​   expect(book).to.have.a.property(​'subjects'​)
   .that.is.an(​'array'​).​with​.lengthOf(2)
​   .and.contains(​'Military art and science -- Early works to 1800'​)
​   .and.contains(​'War -- Early works to 1800'​);
```

It would be nice if we could use the tag structure to craft a simple CSS selector like this:

```js
​   $(​'dcterms​​\\​​:subject rdf​​\\​​:value'​)
```

However, this selector would match another tag that is in the document, which we don’t want.

**File: [pg132.rdf](/tema3-web/pg132)**

```xml
​  <dcterms:subject>
  ​  <rdf:Description rdf:nodeID=​"Nfb797557d91f44c9b0cb80a0d207eaa5"​>
    ​  <dcam:memberOf rdf:resource=​"http://purl.org/dc/terms/LCC"​/>
    ​  <rdf:value>U</rdf:value>
  ​  </rdf:Description>
​  </dcterms:subject>
```

To spot the difference, look at the `<dcam:memberOf>` tags’ `rdf:resource` URLs. 

The ones we want end in `LCSH`, which stands for **Library of Congress Subject Headings**.

These headings are a collection of rich indexing terms used in bibliographic records.

Contrast that with the tag we don’t want to match, which ends in `LCC`.
This stands for **Library of Congress Classification**.

These are *codes* (C) that divide all knowledge into 21 top-level classes (like U for Military Science) with many subclasses. 

Right now we only want the *Subject Headings* (SH).

Here’s the code:

[lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js)

```js
​   book.subjects = $(​'[rdf​​\\​​:resource$="/LCSH"]'​)
​   .parent().find(​'rdf​​\\​​:value'​)
​   .toArray().map(elem => $(elem).text());
```

First, we select the `<dcam:memberOf>` tags of interest with the CSS selector `[rdf\:resource$="/LCSH"]`. 
- The brackets introduce a CSS attribute selector, and the 
- `$=` indicates that we want elements whose `rdf:resource` attribute **ends** with `/LCSH`.

The `.parent` method traverses up to our currently selected elements’ parents. 

In this case, those are the `<rdf:Description>` tags. 

Then we traverse back down using `.find` to locate all of their `<rdf:value>` tags.

We convert the Cheerio selection object into a true Array and use `.map` to get each element’s `text`. 

### Anticipating Format Changes

A problem with parsing external files are the chances that the format will change over time.
An older version of the Project Gutenberg RDF format had its subjects listed like this:

```xml
​   <dcterms:subject>
    <rdf:Description>
    ​   <dcam:memberOf rdf:resource=​"http://purl.org/dc/terms/LCSH"​/>
    ​   <rdf:value>Military art and science -- Early works to 1800</rdf:value>
    ​   <rdf:value>War -- Early works to 1800</rdf:value>
  ​   </rdf:Description>
​   </dcterms:subject>
```

Compare with the current one:

```xml
​   <dcterms:subject>
  ​   <rdf:Description rdf:nodeID=​"N26bb21da0c924e5abcd5809a47f231e7"​>
    ​   <dcam:memberOf rdf:resource=​"http://purl.org/dc/terms/LCSH"​/>
    ​   <rdf:value>Military art and science -- Early works to 1800</rdf:value>
  ​   </rdf:Description>
​   </dcterms:subject>
​   <dcterms:subject>
  ​   <rdf:Description rdf:nodeID=​"N269948d6ecf64b6caf1c15139afd375b"​>
    ​   <rdf:value>War -- Early works to 1800</rdf:value>
    ​   <dcam:memberOf rdf:resource=​"http://purl.org/dc/terms/LCSH"​/>
  ​   </rdf:Description>
​   </dcterms:subject>
```

Instead of finding each subject’s `<rdf:value>` living in its own `<dcterms:subject>` tag, we find them bunched together under a single one. 

Now consider the traversal code we just wrote. 

1. By finding the `/LCSH tag`, 
2. going up to its parent `<rdf:Description>`, and then
3. searching down for `<rdf:value>` tags, 

our code would work with both this earlier data format and the current one (at the time of this writing, anyway).

Whenever you work with third-party data, there’s a chance that it could change over time. 

When it does, your code may or may not continue to work as expected. 

The beauty of testing in these scenarios is that when a data format changes, you can add more tests. 

This gives you confidence that you’re meeting the new demands of the updated data format while still honoring past data.

### Recapping Data Extraction with Cheerio

[test/parse-rdf-test.js]({{page.code}}/test/parse-rdf-test.js)

```js
​ 'use strict';

const fs = require('fs');
const expect = require('chai').expect;
const parseRDF = require('../lib/parse-rdf.js');
const rdf = fs.readFileSync(`${__dirname}/pg132.rdf`);

describe("Chapter 5: Transforming Data and Testing Continuously", () => {
  describe('parseRDF', () => {
    it('should be a function', () => {
      debugger;
      expect(parseRDF).to.be.a('function');
    });
    it('should parse RDF content', () => {
      const book = parseRDF(rdf);
      // https://www.chaijs.com/api/bdd/#method_language-chains
      expect(book).to.be.a('object');

      expect(book).to.have.a.property('id', 132);

      expect(book).to.have.a.property('title', 'The Art of War');

      expect(book).to.have.a.property('authors').that.is.an('array').with.lengthOf(2).
      and.contains('Sunzi, active 6th century B.C.').
      and.contains('Giles, Lionel');

      expect(book).to.have.a.property('subjects').that.is.an('array').with.lengthOf(2).
      and.contains('Military art and science -- Early works to 1800').
      and.contains('War -- Early works to 1800');
    });
  });
});
```

[lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js)

```js
'use strict';
const cheerio = require("cheerio");

module.exports = rdf => {
  const $ = cheerio.load(rdf);
  const book = {};

  /* We're trying to grab the number 132  out of this XML tag:
    <pgterms:ebook rdf:about="ebooks/132">
  */
  book.id = +  // Convert to a number
     $('pgterms\\:ebook') // In CSS, the colon : is used for pseudo selectors (like :hover) that is why we have to escape it
    .attr('rdf:about') // get the rdf:about attribute
    .replace('ebooks/','');
  // Find   <dcterms:title>The Art of War</dcterms:title>
  book.title = $('dcterms\\:title').text();

  book.authors = $('pgterms\\:agent pgterms\\:name')
                 .toArray() // The collection object returned is not a true Array we have to convert it
                 .map(
                   (e)  => { // e is a document node has no text method
                             // that is why we wrap it $(e) to a cheerio object
                     return $(e).text();
                   }
                 );


  book.subjects = $('[rdf\\:resource$="/LCSH"]').
                  parent() // https://api.jquery.com/parent/
                  .find('rdf\\:value') // https://api.jquery.com/find/
                  .toArray()
                  .map(e => $(e).text());
  return book;
};
```

Using this, we can now quickly put together a command-line program to explore some of the other RDF files. Open your editor and enter this:

[rdf-to-json.js]({{page.code}}/rdf-to-json.js)

```js
#!/usr/bin/env node
const fs = require('fs');
const parseRDF = require('./lib/parse-rdf.js');
const rdf = fs.readFileSync(process.argv[2]);
const book = parseRDF(rdf);
console.log(JSON.stringify(book, null, '  '));
```

When calling [`JSON.stringify`](https://developer.mozilla.org/es/docs/Web/JavaScript/Referencia/Objetos_globales/JSON/stringify) we’re passing three arguments: 
1. The second argument (`null`) is an optional replacer function that can be used for filtering
2. The last argument (`' '`) is used to indent nested objects, making the output more human-readable.

Let’s open a terminal in our databases project directory and run it:

```
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5(master)]$ cd databases/
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ls -l rdf-to-json.js 
-rw-r--r--  1 casiano  staff  217  7 nov 13:36 rdf-to-json.js
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ chmod a+x rdf-to-json.js 
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ls -l rdf-to-json.js 
-rwxr-xr-x  1 casiano  staff  217  7 nov 13:36 rdf-to-json.js
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ./rdf-to-json.js ../data/cache/epub//11/pg11.rdf 
{
  "id": 11,
  "title": "Alice's Adventures in Wonderland",
  "authors": [
    "Carroll, Lewis"
  ],
  "subjects": [
    "Fantasy literature"
  ]
}
```


## Processing Data Files Sequentially

Our [lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js)  converts RDF content into JSON documents. 

All that remains is to walk through the Project Gutenberg catalog directory and collect all the JSON documents.

More concretely, we need to do the following:

1. Traverse down the `data/cache/epub` directory looking for files ending in `rdf`.
2. Read each RDF file.
3. Run the RDF content through `parseRDF`.
4. Collect the JSON serialized objects into a single, bulk file for insertion.


The NoSQL database we’ll be using is Elasticsearch, a document datastore that indexes JSON objects.

**GOAL**: We want to transform the Gutenberg data into an intermediate form for bulk import.

[Elasticsearch has a bulk-import API that lets you pull in many records at once](https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/api-reference.html)

Although we could insert them one at a time, it is significantly faster to use the bulk-insert API.

[The format of the file we need to create is described on Elasticsearch’s Bulk API page](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html)

It’s an LDJ file consisting of 
1. **actions** and 
2. the **source objects on which to perform each action**.

In our case, we’re performing `index` operations—that is, 
**inserting new documents into an index**.

Each source object is the book object returned by `parseRDF`.
Here’s an example of an action followed by its source object:

```
​  {​"index"​:{​"_id"​:​"pg11"​}}
​  {​"id"​:11,​"title"​:​"Alice's Adventures in Wonderland"​,​"authors"​:...}
```

And here’s another one:

```
​   {​"index"​:{​"_id"​:​"pg132"​}}
​   {​"id"​:132,​"title"​:​"The Art of War"​,​"authors"​:...}
```

In each case,
1. an action is a JSON object on a line by itself, and 
2. the source object is another JSON object on the next line.

Elasticsearch’s bulk API allows you to chain any number of these together like so:

```
  {​"index"​:{​"_id"​:​"pg11"​}}
​  {​"id"​:11,​"title"​:​"Alice's Adventures in Wonderland"​,​"authors"​:...}
​  {​"index"​:{​"_id"​:​"pg132"​}}
​  {​"id"​:132,​"title"​:​"The Art of War"​,​"authors"​:...}
```


To find and open each of the `RDF` files under the `data/cache/epub` directory, we will use a module called [node-dir](https://github.com/fshost/node-dir).

Install it:

```
​​$ ​​npm​​ ​​install​​ ​​​​node-dir
```

This module comes with a handful of useful methods for walking a directory tree. 

The method we’ll use is 
`readFiles`, 
which sequentially operates on files as it encounters them while walking a directory tree.
<details>
 <summary>
 <b>Argumentos de readFiles</b>
 </summary>
  <ul>
    <li>encoding: file encoding (defaults to 'utf8')</li>
    <li>exclude: a regex pattern or array to specify filenames to ignore</li>
    <li>excludeDir: a regex pattern or array to specify directories to ignore</li>
    <li>match: a regex pattern or array to specify filenames to operate on</li>
    <li>matchDir: a regex pattern or array to specify directories to recurse</li>
    <li>recursive: whether to recurse subdirectories when reading files (defaults to true)</li>
    <li>reverse: sort files in each directory in descending order</li>
    <li>shortName: whether to aggregate only the base filename rather than the full filepath</li>
    <li>sort: sort files in each directory in ascending order (defaults to true)</li>
    <li>doneOnErr: control if done function called on error (defaults to true)</li>
  </ul>
</details>


**[rdf-to-bulk]({{page.code}}/rdf-to-bulk.js)**

```js
// Run as:
//          node rdf-to-bulk.js ../data/cache/epub | head
// or       gulp c5-rdf-to-json-11
'use strict';
const dir = require('node-dir');
const parseRDF = require(__dirname+'/lib/parse-rdf.js');
const dirname = process.argv[2];

const options = {
  match: /\.rdf$/,
  exclude: ['pg0.rdf']
};

// Because the head command closes the pipe after echoing the beginning lines
// we capture error events on the process.stdout stream
process.stdout.on('error', err => process.exit());

dir.readFiles(dirname, options, (err, content, next) => {
  if (err) throw err;
  const doc = parseRDF(content);
  console.log(JSON.stringify({ index: { _id: `pg${doc.id}`} }));
  console.log(JSON.stringify(doc));
  next();
});

```

The `_id` field of each index operation is the unique identifier that Elasticsearch will use for the document.

Here the author has chosen to use the string `pg` followed by the Project Gutenberg ID. This way, if we ever wanted to store documents from another source in the same index, they shouldn’t collide with the Project Gutenberg book data.

Run the program like that:

```
~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ node rdf-to-bulk.js ../data/cache/epub | head
{"index":{"_id":"pg1"}}
{"id":1,"title":"The Declaration of Independence of the United States of America","authors":["Jefferson, Thomas"],"subjects":["United States -- History -- Revolution, 1775-1783 -- Sources","United States. Declaration of Independence"]}
{"index":{"_id":"pg10"}}
{"id":10,"title":"The King James Version of the Bible","authors":[],"subjects":["Bible"]}
{"index":{"_id":"pg100"}}
{"id":100,"title":"The Complete Works of William Shakespeare","authors":["Shakespeare, William"],"subjects":["English drama -- Early modern and Elizabethan, 1500-1600"]}
{"index":{"_id":"pg1000"}}
{"id":1000,"title":"La Divina Commedia di Dante: Complete","authors":["Dante Alighieri"],"subjects":[]}
{"index":{"_id":"pg10000"}}
{"id":10000,"title":"The Magna Carta","authors":["Anonymous"],"subjects":["Constitutional history -- England -- Sources","Magna Carta"]}
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ 
```

Because the head command closes the pipe after echoing the beginning lines, this can sometimes cause Node.js to throw an exception.
We can capture error events on the `process.stdout` stream. That is why we have added  the following line to `rdf-to-bulk.js`:

```js
​ 	process.stdout.on(​'error'​, err => process.exit());
```

Use the following command to capture the output in a new file called `bulk_pg.ldj`.
```
 ​​node​​ ​​rdf-to-bulk.js​​ ​​../data/cache/epub/​​ ​​>​​ ​​../data/bulk_pg.ldj​
```

Esto lleva su tiempo, en mi máquina ocupa mas de 12MB:

```
$ ls -lh data/
total 46664
-rw-r--r--  1 casiano  staff   294B 29 oct  2018 README.md
-rw-r--r--  1 casiano  staff    12M 11 mar  2019 bulk_pg.ldj
-rw-r--r--  1 casiano  staff    10M 20 nov  2018 bulk_result.json
drwxr-xr-x  3 casiano  staff    96B 25 oct  2018 cache
```

## La práctica p9-t3-transforming-data: Extracting Classigication Codes and Sources

* [La práctica p9-t3-transforming-data](/tema3-web/practicas/p9-t3-transforming-data/)

## El Reto: Precios de Hiperdino

* [Escriba un programa que liste los precios de Hiperdino por categoría del producto]({{site.url}}/tema3-web/practicas/p9-t3-transforming-data/reto)
