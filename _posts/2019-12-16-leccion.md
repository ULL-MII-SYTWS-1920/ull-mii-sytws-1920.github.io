---
layout: post
title:  "Clase del Lunes 16/12/2019"
categories: Clases
---

# Clase del Lunes 16/12/2019

## Final de Curso

* [Descripción del calendario](https://drive.google.com/file/d/1EBVRMo7LwDQYdQ4-d7Pbzw84XQeqIa-C/view)
* Entiendo que las clases se acaban el 17/01/2019
* Los exámenes aparece del 20/01 al 01/02. Los de la Web ULL no han actualizado el enlace, pero es este:
  - [Exámenes de la Convocatoria de Enero](https://docs.google.com/document/d/1L1vcYHPtowuP_v-1HLMtJms7S_efD3jHt9H4zrND1IU/edit)
  - Enero: 
     - 24/1/2020
    - 16:00
    - 2.2
    - 31/1/2020
    - 16:00
    - 2.2
  - Junio
    - 3/7/2020
    - 16:00
    - 2.2
  - Septiembre
    - 4/9/2020
    - 16:00
    - 2.2
* Cierre de actas: Viernes 7 de Febrero de 2020 
* Hemos añadido una práctica nueva y el Trabajo Fin de Asignatura
  - [p11-t3-restful]({{site.base_url}}/tema3-web/practicas/p11-t3-restful/)
  - [p12-tfa-user-experience]({{site.base_url}}/tema3-web/practicas/p12-tfa-user-experience/)
* Pueden encontrar las soluciones a las prácticas del autor del libro en este repo [ULL-MII-SYTWS-1920/book-solution-nodejs-the-right-way](https://github.com/ULL-MII-SYTWS-1920/book-solution-nodejs-the-right-way). Debería tener permisos de acceso
* Para cada convocatoria, las prácticas deberían estar entregadas en su mayoría 5 días antes antes de los cierres de actas, que son estos días:
  * Viernes 14 de Febrero 2020
  * Viernes 27 de Marzo 2020
  * Jueves 25 de Junio 2020
  * Miércoles 22 de Julio 2020
  * Jueves 25 de Septiembre 2020

<style>
table, td, th {  
  border: 1px solid #ddd;
  text-align: left;
}

table {
  border-collapse: collapse;
  width: 60%;
}

th, td {
  padding: 1px;
}
</style>
<table>
  <tr>
    <td>
    <img alt="/assets/images/enero2020.png" src="/assets/images/enero2020.png">
    </td>
    <td>
    <img src="/assets/images/instrucciones-calendario.png">
    </td>
  </tr>
</table>

## Recapping the *Transforming Data and Testing Continuously* Lesson

### [test/parse-rdf-test.js]({{page.code}}/test/parse-rdf-test.js)

```js
​ 'use strict';

const fs = require('fs');
const expect = require('chai').expect;
const parseRDF = require('../lib/parse-rdf.js');
const rdf = fs.readFileSync(`${__dirname}/pg132.rdf`);

describe("Chapter 5: Transforming Data and Testing Continuously", () => {
  describe('parseRDF', () => {
    it('should be a function', () => {
      debugger;
      expect(parseRDF).to.be.a('function');
    });
    it('should parse RDF content', () => {
      const book = parseRDF(rdf);
      // https://www.chaijs.com/api/bdd/#method_language-chains
      expect(book).to.be.a('object');

      expect(book).to.have.a.property('id', 132);

      expect(book).to.have.a.property('title', 'The Art of War');

      expect(book).to.have.a.property('authors').that.is.an('array').with.lengthOf(2).
      and.contains('Sunzi, active 6th century B.C.').
      and.contains('Giles, Lionel');

      expect(book).to.have.a.property('subjects').that.is.an('array').with.lengthOf(2).
      and.contains('Military art and science -- Early works to 1800').
      and.contains('War -- Early works to 1800');
    });
  });
});
```

### [lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js)

```js
'use strict';
const cheerio = require("cheerio");

module.exports = rdf => {
  const $ = cheerio.load(rdf);
  const book = {};

  /* We're trying to grab the number 132  out of this XML tag:
    <pgterms:ebook rdf:about="ebooks/132">
  */
  book.id = +  // Convert to a number
     $('pgterms\\:ebook') // In CSS, the colon : is used for pseudo selectors (like :hover) that is why we have to escape it
    .attr('rdf:about') // get the rdf:about attribute
    .replace('ebooks/','');
  // Find   <dcterms:title>The Art of War</dcterms:title>
  book.title = $('dcterms\\:title').text();

  book.authors = $('pgterms\\:agent pgterms\\:name')
                 .toArray() // The collection object returned is not a true Array we have to convert it
                 .map(
                   (e)  => { // e is a document node has no text method
                             // that is why we wrap it $(e) to a cheerio object
                     return $(e).text();
                   }
                 );


  book.subjects = $('[rdf\\:resource$="/LCSH"]').
                  parent() // https://api.jquery.com/parent/
                  .find('rdf\\:value') // https://api.jquery.com/find/
                  .toArray()
                  .map(e => $(e).text());
  return book;
};
```

Using this, we can now quickly put together a command-line program to explore some of the other RDF files. Open your editor and enter this:

### [rdf-to-json.js]({{page.code}}/rdf-to-json.js)

```js
#!/usr/bin/env node
const fs = require('fs');
const parseRDF = require('./lib/parse-rdf.js');
const rdf = fs.readFileSync(process.argv[2]);
const book = parseRDF(rdf);
console.log(JSON.stringify(book, null, '  '));
```

When calling [`JSON.stringify`](https://developer.mozilla.org/es/docs/Web/JavaScript/Referencia/Objetos_globales/JSON/stringify) we’re passing three arguments: 
1. The second argument (`null`) is an optional replacer function that can be used for filtering
2. The last argument (`' '`) is used to indent nested objects, making the output more human-readable.

Let’s open a terminal in our databases project directory and run it:

```
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5(master)]$ cd databases/
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ls -l rdf-to-json.js 
-rw-r--r--  1 casiano  staff  217  7 nov 13:36 rdf-to-json.js
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ chmod a+x rdf-to-json.js 
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ls -l rdf-to-json.js 
-rwxr-xr-x  1 casiano  staff  217  7 nov 13:36 rdf-to-json.js
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ./rdf-to-json.js ../data/cache/epub//11/pg11.rdf 
{
  "id": 11,
  "title": "Alice's Adventures in Wonderland",
  "authors": [
    "Carroll, Lewis"
  ],
  "subjects": [
    "Fantasy literature"
  ]
}
```


### Processing Data Files Sequentially

Our [lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js)  converts RDF content into JSON documents. 

All that remains is to walk through the Project Gutenberg catalog directory and collect all the JSON documents.

More concretely, we need to do the following:

1. Traverse down the `data/cache/epub` directory looking for files ending in `rdf`.
2. Read each RDF file.
3. Run the RDF content through `parseRDF`.
4. Collect the JSON serialized objects into a single, bulk file for insertion.


The NoSQL database we’ll be using is Elasticsearch, a document datastore that indexes JSON objects.

**GOAL**: We want to transform the Gutenberg data into an intermediate form for bulk import.

[Elasticsearch has a bulk-import API that lets you pull in many records at once](https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/api-reference.html)

Although we could insert them one at a time, it is significantly faster to use the bulk-insert API.

[The format of the file we need to create is described on Elasticsearch’s Bulk API page](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html)

It’s an LDJ file consisting of 
1. **actions** and 
2. the **source objects on which to perform each action**.

In our case, we’re performing `index` operations—that is, 
**inserting new documents into an index**.

Each source object is the book object returned by `parseRDF`.
Here’s an example of an action followed by its source object:

```
​  {​"index"​:{​"_id"​:​"pg11"​}}
​  {​"id"​:11,​"title"​:​"Alice's Adventures in Wonderland"​,​"authors"​:...}
```

And here’s another one:

```
​   {​"index"​:{​"_id"​:​"pg132"​}}
​   {​"id"​:132,​"title"​:​"The Art of War"​,​"authors"​:...}
```

In each case,
1. an action is a JSON object on a line by itself, and 
2. the source object is another JSON object on the next line.

Elasticsearch’s bulk API allows you to chain any number of these together like so:

```
  {​"index"​:{​"_id"​:​"pg11"​}}
​  {​"id"​:11,​"title"​:​"Alice's Adventures in Wonderland"​,​"authors"​:...}
​  {​"index"​:{​"_id"​:​"pg132"​}}
​  {​"id"​:132,​"title"​:​"The Art of War"​,​"authors"​:...}
```


To find and open each of the `RDF` files under the `data/cache/epub` directory, we will use a module called [node-dir](https://github.com/fshost/node-dir).

Install it:

```
​​$ ​​npm​​ ​​install​​ ​​​​node-dir
```

This module comes with a handful of useful methods for walking a directory tree. 

The method we’ll use is 
`readFiles`, 
which sequentially operates on files as it encounters them while walking a directory tree.
<details>
 <summary>
 <b>Argumentos de readFiles</b>
 </summary>
  <ul>
    <li>encoding: file encoding (defaults to 'utf8')</li>
    <li>exclude: a regex pattern or array to specify filenames to ignore</li>
    <li>excludeDir: a regex pattern or array to specify directories to ignore</li>
    <li>match: a regex pattern or array to specify filenames to operate on</li>
    <li>matchDir: a regex pattern or array to specify directories to recurse</li>
    <li>recursive: whether to recurse subdirectories when reading files (defaults to true)</li>
    <li>reverse: sort files in each directory in descending order</li>
    <li>shortName: whether to aggregate only the base filename rather than the full filepath</li>
    <li>sort: sort files in each directory in ascending order (defaults to true)</li>
    <li>doneOnErr: control if done function called on error (defaults to true)</li>
  </ul>
</details>


**[rdf-to-bulk]({{page.code}}/rdf-to-bulk.js)**

```js
// Run as:
//          node rdf-to-bulk.js ../data/cache/epub | head
// or       gulp c5-rdf-to-json-11
'use strict';
const dir = require('node-dir');
const parseRDF = require(__dirname+'/lib/parse-rdf.js');
const dirname = process.argv[2];

const options = {
  match: /\.rdf$/,
  exclude: ['pg0.rdf']
};

// Because the head command closes the pipe after echoing the beginning lines
// we capture error events on the process.stdout stream
process.stdout.on('error', err => process.exit());

dir.readFiles(dirname, options, (err, content, next) => {
  if (err) throw err;
  const doc = parseRDF(content);
  console.log(JSON.stringify({ index: { _id: `pg${doc.id}`} }));
  console.log(JSON.stringify(doc));
  next();
});

```

The `_id` field of each index operation is the unique identifier that Elasticsearch will use for the document.

Here the author has chosen to use the string `pg` followed by the Project Gutenberg ID. This way, if we ever wanted to store documents from another source in the same index, they shouldn’t collide with the Project Gutenberg book data.

Run the program like that:

```
~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ node rdf-to-bulk.js ../data/cache/epub | head
{"index":{"_id":"pg1"}}
{"id":1,"title":"The Declaration of Independence of the United States of America","authors":["Jefferson, Thomas"],"subjects":["United States -- History -- Revolution, 1775-1783 -- Sources","United States. Declaration of Independence"]}
{"index":{"_id":"pg10"}}
{"id":10,"title":"The King James Version of the Bible","authors":[],"subjects":["Bible"]}
{"index":{"_id":"pg100"}}
{"id":100,"title":"The Complete Works of William Shakespeare","authors":["Shakespeare, William"],"subjects":["English drama -- Early modern and Elizabethan, 1500-1600"]}
{"index":{"_id":"pg1000"}}
{"id":1000,"title":"La Divina Commedia di Dante: Complete","authors":["Dante Alighieri"],"subjects":[]}
{"index":{"_id":"pg10000"}}
{"id":10000,"title":"The Magna Carta","authors":["Anonymous"],"subjects":["Constitutional history -- England -- Sources","Magna Carta"]}
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ 
```

Because the head command closes the pipe after echoing the beginning lines, this can sometimes cause Node.js to throw an exception.
We can capture error events on the `process.stdout` stream. That is why we have added  the following line to `rdf-to-bulk.js`:

```js
​ 	process.stdout.on(​'error'​, err => process.exit());
```

Use the following command to capture the output in a new file called `bulk_pg.ldj`.
```
 ​​node​​ ​​rdf-to-bulk.js​​ ​​../data/cache/epub/​​ ​​>​​ ​​../data/bulk_pg.ldj​
```

Esto lleva su tiempo, en mi máquina ocupa mas de 12MB:

```
$ ls -lh data/
total 46664
-rw-r--r--  1 casiano  staff   294B 29 oct  2018 README.md
-rw-r--r--  1 casiano  staff    12M 11 mar  2019 bulk_pg.ldj
-rw-r--r--  1 casiano  staff    10M 20 nov  2018 bulk_result.json
drwxr-xr-x  3 casiano  staff    96B 25 oct  2018 cache
```

### Descripción de la práctica p9-t3-transforming-data: Extracting Classification Codes and Sources

* [La práctica p9-t3-transforming-data](/tema3-web/practicas/p9-t3-transforming-data/)

## El Reto: Precios de Hiperdino

* [Escriba un programa que liste los precios de Hiperdino por categoría del producto]({{site.url}}/tema3-web/practicas/p9-t3-transforming-data/reto)