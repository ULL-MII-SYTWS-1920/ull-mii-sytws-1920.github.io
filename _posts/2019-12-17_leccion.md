---
layout: post
title:  "Clase del Miércoles 18/12/2019"
categories: Clases
---

# Clase del Miércoles 18/12/2019

## Final de Curso

* [Descripción del calendario](https://drive.google.com/file/d/1EBVRMo7LwDQYdQ4-d7Pbzw84XQeqIa-C/view)
* Entiendo que las clases se acaban el 17/01/2019
* Los exámenes aparece del 20/01 al 01/02. Los de la Web ULL no han actualizado el enlace, pero es este:
  - [Exámenes de la Convocatoria de Enero](https://docs.google.com/document/d/1L1vcYHPtowuP_v-1HLMtJms7S_efD3jHt9H4zrND1IU/edit)
  - Enero: 
     - 24/1/2020
    - 16:00
    - 2.2
    - 31/1/2020
    - 16:00
    - 2.2
  - Junio
    - 3/7/2020
    - 16:00
    - 2.2
  - Septiembre
    - 4/9/2020
    - 16:00
    - 2.2
* Cierre de actas: Viernes 7 de Febrero de 2020 
* Hemos añadido una práctica nueva y el Trabajo Fin de Asignatura
  - [p11-t3-restful]({{site.base_url}}/tema3-web/practicas/p11-t3-restful/)
  - [p12-tfa-user-experience]({{site.base_url}}/tema3-web/practicas/p12-tfa-user-experience/)
* Pueden encontrar las soluciones a las prácticas del autor del libro en este repo [ULL-MII-SYTWS-1920/book-solution-nodejs-the-right-way](https://github.com/ULL-MII-SYTWS-1920/book-solution-nodejs-the-right-way). Debería tener permisos de acceso
* Para cada convocatoria, las prácticas deberían estar entregadas en su mayoría 5 días antes antes de los cierres de actas, que son estos días:
  * Viernes 14 de Febrero 2020
  * Viernes 27 de Marzo 2020
  * Jueves 25 de Junio 2020
  * Miércoles 22 de Julio 2020
  * Jueves 25 de Septiembre 2020

<style>
table, td, th {  
  border: 1px solid #ddd;
  text-align: left;
}

table {
  border-collapse: collapse;
  width: 60%;
}

th, td {
  padding: 1px;
}
</style>
<table>
  <tr>
    <td>
    <img alt="/assets/images/enero2020.png" src="/assets/images/enero2020.png">
    </td>
    <td>
    <img src="/assets/images/instrucciones-calendario.png">
    </td>
  </tr>
</table>

## Recapping the *Transforming Data and Testing Continuously* Lesson

### [test/parse-rdf-test.js]({{page.code}}/test/parse-rdf-test.js)

```js
​ 'use strict';

const fs = require('fs');
const expect = require('chai').expect;
const parseRDF = require('../lib/parse-rdf.js');
const rdf = fs.readFileSync(`${__dirname}/pg132.rdf`);

describe("Chapter 5: Transforming Data and Testing Continuously", () => {
  describe('parseRDF', () => {
    it('should be a function', () => {
      debugger;
      expect(parseRDF).to.be.a('function');
    });
    it('should parse RDF content', () => {
      const book = parseRDF(rdf);
      // https://www.chaijs.com/api/bdd/#method_language-chains
      expect(book).to.be.a('object');

      expect(book).to.have.a.property('id', 132);

      expect(book).to.have.a.property('title', 'The Art of War');

      expect(book).to.have.a.property('authors').that.is.an('array').with.lengthOf(2).
      and.contains('Sunzi, active 6th century B.C.').
      and.contains('Giles, Lionel');

      expect(book).to.have.a.property('subjects').that.is.an('array').with.lengthOf(2).
      and.contains('Military art and science -- Early works to 1800').
      and.contains('War -- Early works to 1800');
    });
  });
});
```

### [lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js)

```js
'use strict';
const cheerio = require("cheerio");

module.exports = rdf => {
  const $ = cheerio.load(rdf);
  const book = {};

  /* We're trying to grab the number 132  out of this XML tag:
    <pgterms:ebook rdf:about="ebooks/132">
  */
  book.id = +  // Convert to a number
     $('pgterms\\:ebook') // In CSS, the colon : is used for pseudo selectors (like :hover) that is why we have to escape it
    .attr('rdf:about') // get the rdf:about attribute
    .replace('ebooks/','');
  // Find   <dcterms:title>The Art of War</dcterms:title>
  book.title = $('dcterms\\:title').text();

  book.authors = $('pgterms\\:agent pgterms\\:name')
                 .toArray() // The collection object returned is not a true Array we have to convert it
                 .map(
                   (e)  => { // e is a document node has no text method
                             // that is why we wrap it $(e) to a cheerio object
                     return $(e).text();
                   }
                 );


  book.subjects = $('[rdf\\:resource$="/LCSH"]').
                  parent() // https://api.jquery.com/parent/
                  .find('rdf\\:value') // https://api.jquery.com/find/
                  .toArray()
                  .map(e => $(e).text());
  return book;
};
```

Using this, we can now quickly put together a command-line program to explore some of the other RDF files. Open your editor and enter this:

### [rdf-to-json.js]({{page.code}}/rdf-to-json.js)

```js
#!/usr/bin/env node
const fs = require('fs');
const parseRDF = require('./lib/parse-rdf.js');
const rdf = fs.readFileSync(process.argv[2]);
const book = parseRDF(rdf);
console.log(JSON.stringify(book, null, '  '));
```

When calling [`JSON.stringify`](https://developer.mozilla.org/es/docs/Web/JavaScript/Referencia/Objetos_globales/JSON/stringify) we’re passing three arguments: 
1. The second argument (`null`) is an optional replacer function that can be used for filtering
2. The last argument (`' '`) is used to indent nested objects, making the output more human-readable.

Let’s open a terminal in our databases project directory and run it:

```
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5(master)]$ cd databases/
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ls -l rdf-to-json.js 
-rw-r--r--  1 casiano  staff  217  7 nov 13:36 rdf-to-json.js
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ chmod a+x rdf-to-json.js 
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ls -l rdf-to-json.js 
-rwxr-xr-x  1 casiano  staff  217  7 nov 13:36 rdf-to-json.js
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ./rdf-to-json.js ../data/cache/epub//11/pg11.rdf 
{
  "id": 11,
  "title": "Alice's Adventures in Wonderland",
  "authors": [
    "Carroll, Lewis"
  ],
  "subjects": [
    "Fantasy literature"
  ]
}
```


### Processing Data Files Sequentially

Our [lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js)  converts RDF content into JSON documents. 

All that remains is to walk through the Project Gutenberg catalog directory and collect all the JSON documents.

More concretely, we need to do the following:

1. Traverse down the `data/cache/epub` directory looking for files ending in `rdf`.
2. Read each RDF file.
3. Run the RDF content through `parseRDF`.
4. Collect the JSON serialized objects into a single, bulk file for insertion.


The NoSQL database we’ll be using is Elasticsearch, a document datastore that indexes JSON objects.

**GOAL**: We want to transform the Gutenberg data into an intermediate form for bulk import.

[Elasticsearch has a bulk-import API that lets you pull in many records at once](https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/api-reference.html)

Although we could insert them one at a time, it is significantly faster to use the bulk-insert API.

[The format of the file we need to create is described on Elasticsearch’s Bulk API page](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html)

It’s an LDJ file consisting of 
1. **actions** and 
2. the **source objects on which to perform each action**.

In our case, we’re performing `index` operations—that is, 
**inserting new documents into an index**.

Each source object is the book object returned by `parseRDF`.
Here’s an example of an action followed by its source object:

```
​  {​"index"​:{​"_id"​:​"pg11"​}}
​  {​"id"​:11,​"title"​:​"Alice's Adventures in Wonderland"​,​"authors"​:...}
```

And here’s another one:

```
​   {​"index"​:{​"_id"​:​"pg132"​}}
​   {​"id"​:132,​"title"​:​"The Art of War"​,​"authors"​:...}
```

In each case,
1. an action is a JSON object on a line by itself, and 
2. the source object is another JSON object on the next line.

Elasticsearch’s bulk API allows you to chain any number of these together like so:

```
  {​"index"​:{​"_id"​:​"pg11"​}}
​  {​"id"​:11,​"title"​:​"Alice's Adventures in Wonderland"​,​"authors"​:...}
​  {​"index"​:{​"_id"​:​"pg132"​}}
​  {​"id"​:132,​"title"​:​"The Art of War"​,​"authors"​:...}
```


To find and open each of the `RDF` files under the `data/cache/epub` directory, we will use a module called [node-dir](https://github.com/fshost/node-dir).

Install it:

```
​​$ ​​npm​​ ​​install​​ ​​​​node-dir
```

This module comes with a handful of useful methods for walking a directory tree. 

The method we’ll use is 
`readFiles`, 
which sequentially operates on files as it encounters them while walking a directory tree.
<details>
 <summary>
 <b>Argumentos de readFiles</b>
 </summary>
  <ul>
    <li>encoding: file encoding (defaults to 'utf8')</li>
    <li>exclude: a regex pattern or array to specify filenames to ignore</li>
    <li>excludeDir: a regex pattern or array to specify directories to ignore</li>
    <li>match: a regex pattern or array to specify filenames to operate on</li>
    <li>matchDir: a regex pattern or array to specify directories to recurse</li>
    <li>recursive: whether to recurse subdirectories when reading files (defaults to true)</li>
    <li>reverse: sort files in each directory in descending order</li>
    <li>shortName: whether to aggregate only the base filename rather than the full filepath</li>
    <li>sort: sort files in each directory in ascending order (defaults to true)</li>
    <li>doneOnErr: control if done function called on error (defaults to true)</li>
  </ul>
</details>


**[rdf-to-bulk]({{page.code}}/rdf-to-bulk.js)**

```js
// Run as:
//          node rdf-to-bulk.js ../data/cache/epub | head
// or       gulp c5-rdf-to-json-11
'use strict';
const dir = require('node-dir');
const parseRDF = require(__dirname+'/lib/parse-rdf.js');
const dirname = process.argv[2];

const options = {
  match: /\.rdf$/,
  exclude: ['pg0.rdf']
};

// Because the head command closes the pipe after echoing the beginning lines
// we capture error events on the process.stdout stream
process.stdout.on('error', err => process.exit());

dir.readFiles(dirname, options, (err, content, next) => {
  if (err) throw err;
  const doc = parseRDF(content);
  console.log(JSON.stringify({ index: { _id: `pg${doc.id}`} }));
  console.log(JSON.stringify(doc));
  next();
});

```

The `_id` field of each index operation is the unique identifier that Elasticsearch will use for the document.

Here the author has chosen to use the string `pg` followed by the Project Gutenberg ID. This way, if we ever wanted to store documents from another source in the same index, they shouldn’t collide with the Project Gutenberg book data.

Run the program like that:

```
~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ node rdf-to-bulk.js ../data/cache/epub | head
{"index":{"_id":"pg1"}}
{"id":1,"title":"The Declaration of Independence of the United States of America","authors":["Jefferson, Thomas"],"subjects":["United States -- History -- Revolution, 1775-1783 -- Sources","United States. Declaration of Independence"]}
{"index":{"_id":"pg10"}}
{"id":10,"title":"The King James Version of the Bible","authors":[],"subjects":["Bible"]}
{"index":{"_id":"pg100"}}
{"id":100,"title":"The Complete Works of William Shakespeare","authors":["Shakespeare, William"],"subjects":["English drama -- Early modern and Elizabethan, 1500-1600"]}
{"index":{"_id":"pg1000"}}
{"id":1000,"title":"La Divina Commedia di Dante: Complete","authors":["Dante Alighieri"],"subjects":[]}
{"index":{"_id":"pg10000"}}
{"id":10000,"title":"The Magna Carta","authors":["Anonymous"],"subjects":["Constitutional history -- England -- Sources","Magna Carta"]}
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ 
```

Because the head command closes the pipe after echoing the beginning lines, this can sometimes cause Node.js to throw an exception.
We can capture error events on the `process.stdout` stream. That is why we have added  the following line to `rdf-to-bulk.js`:

```js
​ 	process.stdout.on(​'error'​, err => process.exit());
```

Use the following command to capture the output in a new file called `bulk_pg.ldj`.
```
 ​​node​​ ​​rdf-to-bulk.js​​ ​​../data/cache/epub/​​ ​​>​​ ​​../data/bulk_pg.ldj​
```

Esto lleva su tiempo, en mi máquina ocupa mas de 12MB:

```
$ ls -lh data/
total 46664
-rw-r--r--  1 casiano  staff   294B 29 oct  2018 README.md
-rw-r--r--  1 casiano  staff    12M 11 mar  2019 bulk_pg.ldj
-rw-r--r--  1 casiano  staff    10M 20 nov  2018 bulk_result.json
drwxr-xr-x  3 casiano  staff    96B 25 oct  2018 cache
```

### Descripción de la práctica p9-t3-transforming-data: Extracting Classification Codes and Sources

* [La práctica p9-t3-transforming-data](/tema3-web/practicas/p9-t3-transforming-data/)

## El Reto: Precios de Hiperdino

* [Escriba un programa que liste los precios de Hiperdino por categoría del producto]({{site.url}}/tema3-web/practicas/p9-t3-transforming-data/reto)

## Chapter 6: Commanding DataBases

En este capítulo construiremos un programa de línea de comandos `esclu` 
para interactuar con la base de datos Elasticsearch.

Elasticsearch is a schema-free, RESTful, NoSQL database that stores and indexes JSON documents over HTTP.

Nuestro programa `esclu` hará requests REST al servidor de ElasticSearch y este retornará un JSON con la respuesta al 
request. Sigue un ejemplo de ejecución. 

Puesto que la salida es un JSON muy grande, la hemos canalizado a un programa 
[`jq`](https://stedolan.github.io/jq/tutorial/) (JSON query language):


```
$ ./esclu query authors:Twain AND subjects:children | jq .hits.hits[1]._source
{
  "id": 1837,
  "title": "The Prince and the Pauper",
  "authors": [
    "Twain, Mark"
  ],
  "subjects": [
    "Edward VI, King of England, 1537-1553 -- Fiction",
    "Lookalikes -- Fiction",
    "London (England) -- Fiction",
    "Historical fiction",
    "Impostors and imposture -- Fiction",
    "Boys -- Fiction",
    "Princes -- Fiction",
    "Social classes -- Fiction",
    "Poor children -- Fiction"
  ]
}
```

Lo que nos permite el DSL `jq` es navegar a través del JSON y obtener el campo en el que estamos interesados. En
el ejemplo anterior la salida del comando `./esclu query authors:Twain AND subjects:children` es un JSON  como este:

```
{
  "took": 37,
  ...
  "hits": {
    "total": 41,
    "max_score": 9.074243,
    "hits": [
      { ... },
      {
        ...
        "_id": "pg7195",
        "_source": {
            "id": 1837,
            "title": "The Prince and the Pauper",
          ...
        }
      },
      ...
  }
```

De manera que con la expresión  `jq .hits.hits[1]._source` describimos un elemento concreto del JSON.

[Elasticsearch](https://es.wikipedia.org/wiki/Elasticsearch) es un servidor de búsqueda basado en Lucene. Provee un motor de búsqueda de texto completo, distribuido y con [capacidad de multi-tenencia](https://es.wikipedia.org/wiki/Tenencia_m%C3%BAltiple) con una interfaz web RESTful y con documentos JSON. Elasticsearch está desarrollado en Java y está publicado como código abierto bajo las condiciones de la licencia Apache.

Usando el comando `esclu bulk` cargaremos los documentos JSON generados en el capítulo
anterior en la base de datos de Elasticsearch:

```
$ ./esclu bulk ../data/bulk_pg.ldj -i books -t book > ../data/bulk_result.json
```


![/assets/images/ch6-xml-2-json-ch6-2-es.png](/assets/images/ch6-xml-2-json-ch6-2-es.png)

### Algunas características de Elasticsearch

#### Queries

Elasticsearch utiliza Query DSL (Lenguaje de dominio específico) para realizar las consultas a los documentos indexados. Es un lenguaje sumamente flexible y de gran alcance, además de simple, que permite conocer y explorar los datos de la mejor manera. Al ser utilizado a través de una interfaz de tipo JSON, las consultas son muy sencillas de leer y, lo más importante, de depurar.

* [Useful Elasticsearch Example Queries](https://dzone.com/articles/23-useful-elasticsearch-example-queries) DZone

#### Clusters

* [Creating an Elasticsearch Cluster: Getting Started](https://logz.io/blog/elasticsearch-cluster-tutorial/)
* [Shard](https://en.wikipedia.org/wiki/Shard_(database_architecture))
    - A database shard is a horizontal partition of data in a database or search engine. Each individual partition is referred to as a shard or database shard. Each shard is held on a separate database server instance, to spread load.
    - Some data within a database remains present in all shards but some appears only in a single shard. Each shard (or server) acts as the single source for this subset of data
    - ![sharding in mongodb](https://docs.mongodb.com/v3.0/_images/sharded-collection.png)
    -  [¿Cómo se reparten? Usando las llamadas "*shard keys*" o *claves de repartición*. Cada partición contiene un intervalo de claves (Clave Mínima, Clave Máxima). Se habla de "*partición basada en rangos*". La *Index Big Table* de Google utiliza una idea similar. ](http://gpd.sip.ucm.es/rafa/docencia/nosql/Sharding.html)
    - [Tutorial MongoDB. Explicando el sharding con una baraja de cartas](https://charlascylon.com/2014-01-30-tutorial-mongodb-explicando-el-sharding-con-una)

#### REST

REST is an acronym that stands for Representational State Transfer.

* [REST en http://crguezl.github.io/apuntes-ruby/](http://crguezl.github.io/apuntes-ruby/node566.html)

When an API is RESTful, it is HTTP-based and its resources are identified by their URLs.

Requesting or making a change to a resource comes down to issuing an HTTP request using the particular method that matches your intent.

For example, the HTTP GET method retrieves a resource, and HTTP PUT sends a resource to be saved.

### Prerequistios para la Instalación de Elastic Search

Elasticsearch is built on Java 8.

Instructions on how to install Java 8 are available on [Oracle’s website](https://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html)

You can run `java -version` 
from the command line to confirm that Java is installed and ready.

```
$ java --version
java 9.0.4
Java(TM) SE Runtime Environment (build 9.0.4+11)
Java HotSpot(TM) 64-Bit Server VM (build 9.0.4+11, mixed mode)
```

### Instalación de ElasticSearch

Una forma de instalarse ElasticSearch es ir a la página de descargas:

* [https://www.elastic.co/es/downloads/](https://www.elastic.co/es/downloads/)

La versión que se usa en el libro es la 5.2 que se puede descargar desde aquí:

* [https://www.elastic.co/es/downloads/past-releases/elasticsearch-5-2-2](https://www.elastic.co/es/downloads/past-releases/elasticsearch-5-2-2
)

Aquí se puede encontrar una [guía de inicio rápido.](https://www.elastic.co/es/start).

Esta es la versión que he usado en mi instalación, [la 6.4.2](https://www.elastic.co/es/downloads/past-releases/elasticsearch-6-4-2):

```
$ elasticsearch --version
Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
Version: 6.4.2, Build: default/tar/04711c2/2018-09-26T13:34:09.098244Z, JVM: 9.0.4
```

La version en Diciembre de 2019  es la [7.5.0](https://www.elastic.co/es/downloads/elasticsearch)

Once you download the archive, 
unzip it 
and run `bin/elasticsearch` from the command line. 

You should see a lot of output containing something like the following (much of the output is omitted here for brevity).

```
$ bin/elasticsearch
[INFO ][o.e.n.Node ] [] initializing ...
... many lines omitted ...
[INFO ][o.e.h.HttpServer ] [kAh7Q7Z] publish_address {127.0.0.1:9200},
    bound_addresses {[::1]:9200}, {127.0.0.1:9200}
[INFO ][o.e.n.Node            ] [kAh7Q7Z] started
[INFO ][o.e.g.GatewayService  ] [kAh7Q7Z] recovered [0] indices into
    cluster_state
```

Notice the **`publish_address`** and **`bound_addresses`** listed toward the end of the output. 
By default, **Elasticsearch binds TCP port 9200 for its HTTP endpoint**.

You can specify a lot of settings when setting up an Elasticsearch cluster. By default, is running in development mode.

A full discussion of the Elasticsearch cluster settings for version 5.2  is [Elastic’s Important System Configuration 5.2 page](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/system-config.html).
The same instructions [for the current version are here](https://www.elastic.co/guide/en/elasticsearch/reference/current/system-config.html)

To have Elasticsearch in the PATH, I have added a small script in my `~/.bash_profile`:

```
[~/campus-virtual/1819/ca1819/practicas(master)]$ cat ~/.bash_profile | sed -ne '/elastic/,/^$/p'
source ~/bin/elasticsearch-set
```

With this contents:

```
[~/campus-virtual/1819/ca1819/practicas(master)]$ cat ~/bin/elastic-search-set
export ES_HOME=~/Applications/elasticsearch-6.4.2
export PATH=$ES_HOME/bin:$PATH
```

### Running ElasticSearch

Let us see where `elasticsearch` is installed:

```
[~]$ which elasticsearch
/Users/casiano/Applications/elasticsearch-6.4.2/bin/elasticsearch
```

Let us execute `elasticsearch`in development mode.
The flow of output when executed is overwhelming:

```
[~/sol-nodejs-the-right-way(master)]$ elasticsearch
[Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2019-12-15T11:28:46,903][INFO ][o.e.n.Node               ] [] initializing ...
[2019-12-15T11:28:47,442][INFO ][o.e.e.NodeEnvironment    ] [9jAGWs_] using [1] data paths, mounts [[/ (/dev/disk1s1)]], net usable_space [19.5gb], net total_space [233.5gb], types [apfs]
[2019-12-15T11:28:47,445][INFO ][o.e.e.NodeEnvironment    ] [9jAGWs_] heap size [990.7mb], compressed ordinary object pointers [true]
[2019-12-15T11:28:47,740][INFO ][o.e.n.Node               ] [9jAGWs_] node name derived from node ID [9jAGWs_uQGmUPF4RyFkjTw]; set [node.name] to override
[2019-12-15T11:28:47,740][INFO ][o.e.n.Node               ] [9jAGWs_] version[6.4.2], pid[84005], build[default/tar/04711c2/2018-09-26T13:34:09.098244Z], OS[Mac OS X/10.13.6/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/9.0.4/9.0.4+11]
[2019-12-15T11:28:47,741][INFO ][o.e.n.Node               ] [9jAGWs_] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/var/folders/5g/l7k4y5112612gk6qxzy6jwv40000gn/T/elasticsearch.JiyCf7BF, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Djava.locale.providers=COMPAT, -Des.path.home=/Users/casiano/Applications/elasticsearch-6.4.2, -Des.path.conf=/Users/casiano/Applications/elasticsearch-6.4.2/config, -Des.distribution.flavor=default, -Des.distribution.type=tar]
[2019-12-15T11:28:53,337][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [aggs-matrix-stats]
[2019-12-15T11:28:53,338][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [analysis-common]
[2019-12-15T11:28:53,339][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [ingest-common]
[2019-12-15T11:28:53,342][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [lang-expression]
[2019-12-15T11:28:53,342][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [lang-mustache]
[2019-12-15T11:28:53,343][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [lang-painless]
[2019-12-15T11:28:53,343][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [mapper-extras]
[2019-12-15T11:28:53,343][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [parent-join]
[2019-12-15T11:28:53,344][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [percolator]
[2019-12-15T11:28:53,344][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [rank-eval]
[2019-12-15T11:28:53,344][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [reindex]
[2019-12-15T11:28:53,345][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [repository-url]
[2019-12-15T11:28:53,346][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [transport-netty4]
[2019-12-15T11:28:53,346][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [tribe]
[2019-12-15T11:28:53,347][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-core]
[2019-12-15T11:28:53,348][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-deprecation]
[2019-12-15T11:28:53,348][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-graph]
[2019-12-15T11:28:53,348][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-logstash]
[2019-12-15T11:28:53,348][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-ml]
[2019-12-15T11:28:53,349][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-monitoring]
[2019-12-15T11:28:53,350][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-rollup]
[2019-12-15T11:28:53,350][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-security]
[2019-12-15T11:28:53,351][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-sql]
[2019-12-15T11:28:53,351][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-upgrade]
[2019-12-15T11:28:53,351][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-watcher]
[2019-12-15T11:28:53,352][INFO ][o.e.p.PluginsService     ] [9jAGWs_] no plugins loaded
[2019-12-15T11:29:05,189][INFO ][o.e.x.s.a.s.FileRolesStore] [9jAGWs_] parsed [0] roles from file [/Users/casiano/Applications/elasticsearch-6.4.2/config/roles.yml]
[2019-12-15T11:29:06,667][INFO ][o.e.x.m.j.p.l.CppLogMessageHandler] [controller/84063] [Main.cc@109] controller (64 bit): Version 6.4.2 (Build 660eefe6f2ea55) Copyright (c) 2018 Elasticsearch BV
[2019-12-15T11:29:08,149][DEBUG][o.e.a.ActionModule       ] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security
[2019-12-15T11:29:08,701][INFO ][o.e.d.DiscoveryModule    ] [9jAGWs_] using discovery type [zen]
[2019-12-15T11:29:10,495][INFO ][o.e.n.Node               ] [9jAGWs_] initialized
[2019-12-15T11:29:10,496][INFO ][o.e.n.Node               ] [9jAGWs_] starting ...
[2019-12-15T11:29:10,938][INFO ][o.e.t.TransportService   ] [9jAGWs_] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
[2019-12-15T11:29:14,113][INFO ][o.e.c.s.MasterService    ] [9jAGWs_] zen-disco-elected-as-master ([0] nodes joined)[, ], reason: new_master {9jAGWs_}{9jAGWs_uQGmUPF4RyFkjTw}{2g1w5LQXSZ6gPKwgps9psw}{127.0.0.1}{127.0.0.1:9300}{ml.machine_memory=8589934592, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true}
[2019-12-15T11:29:14,121][INFO ][o.e.c.s.ClusterApplierService] [9jAGWs_] new_master {9jAGWs_}{9jAGWs_uQGmUPF4RyFkjTw}{2g1w5LQXSZ6gPKwgps9psw}{127.0.0.1}{127.0.0.1:9300}{ml.machine_memory=8589934592, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true}, reason: apply cluster state (from master [master {9jAGWs_}{9jAGWs_uQGmUPF4RyFkjTw}{2g1w5LQXSZ6gPKwgps9psw}{127.0.0.1}{127.0.0.1:9300}{ml.machine_memory=8589934592, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true} committed version [1] source [zen-disco-elected-as-master ([0] nodes joined)[, ]]])
[2019-12-15T11:29:14,175][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [9jAGWs_] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}
```

We can see in the last line that is listening at 9200:

```
[2019-12-15T11:29:14,175][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [9jAGWs_] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}
```

Now we can use insomnia or any other HTTP REST client to make queries to the elasticsearch server:

![assets/images/insomnia-elasticsearch-1.png](/assets/images/insomnia-elasticsearch-1.png)

* [Useful Elasticsearch Example Queries](https://dzone.com/articles/23-useful-elasticsearch-example-queries) DZone

### Creating a Command-Line Program in Node.js with Commander

Now, we are going to start to build a command-line program that provides access to the  Elasticsearch
server.

We’ll start by creating a `package.json` inside the folder `esclu`:

```
$ mkdir esclu
$ cd esclu
$ npm init -f
```

### Introducing the Commander and Request Modules

We’re going to use a module called `Commander`, which makes it easy to construct elaborate and powerful command-line tools in Node.js. We will use the module `request` to write our requests to the elasticsearch server:

```
npm i commander request
```

* [Commander at GitHub](https://github.com/tj/commander.js)
* [Commander: Teacher examples](https://github.com/ULL-MII-CA-1819/commander-examples)
* [Commander: Examples of use](https://github.com/tj/commander.js/tree/master/examples)
* YouTube Video [commander.js, npm scopes, and node.js cli's (part 1)](https://youtu.be/jj_B4e6F454)

#### Alternatives to Commander and Request

* [yargs](https://github.com/yargs/yargs)
* [node-fetch](https://github.com/bitinn/node-fetch)This module implements the Fetch API

###  Adding a Command to Your program




#### URLs in Elasticsearch RESTful Search Engine

In Elasticsearch, the RESTful resources are JSON documents.

Each Elasticsearch document:

* lives in an **index** and
* has a **type**, which defines a class of related documents.

To construct a URL for an Elasticsearch document,
* first you append which index you’re interested in (if any) and then
* optionally the type of object you’re interested in, separated by slashes.

To get information about your whole cluster, you could make an HTTP `GET` request to the root: http://localhost:9200/.

To request information about the index named `books`, you would `GET` http://localhost:9200/books.

* See [Index vs. Type](https://www.elastic.co/blog/index-vs-type) article:
    - *In the past we tried to make elasticsearch easier to understand by building an analogy with relational databases: *indices* would be like a database, and *types* like a table in a database.*
    - *This was a mistake: the way data is stored is so different that any comparisons can hardly make sense, and this ultimately led to an overuse of types in cases where they were more harmful than helpful.*

[See the code of `fullUrl`](https://github.com/ULL-MII-CA-1819/nodejs-the-right-way/blob/8cced9a5546c794a08a3670674aabcc9602e20dd/commanding-databases-chapter-6/esclu/index.js#L8-L17)


#### Adding a command in Commander.js

Adding a command in commander.js consists of three steps:

1. Specifying the command name and parameters `url [path]`
    1. Required arguments should be surrounded by angle brackets `<file>`
    2. Optional arguments should be surrounded by square brackets `[path]`
2. Providing a description `'generate the URL for the options and path (default is /)'`
3. Setting up an action callback `(path= "/") => console.log(fullUrl(path))`

See the code that [adds the command `esclu url path`](https://github.com/ULL-MII-CA-1819/nodejs-the-right-way/blob/master/commanding-databases-chapter-6/esclu/index.js#L40-L43)

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu
Usage: esclu [options] <command> [...]

Elasticsearch command line utilities

Options:
  -V, --version          output the version number
  -o, --host <hostname>  hostname [localhost] (default: "localhost")
  -p, --port <number>    port number [9200] (default: "9200")
  -j, --json             format output as json
  -i, --index <name>     which index to use
  -t, --type <type>      default type for bulk operations
  -h, --help             output usage information

Commands:
  url [path]             generate the URL for the options and path (default is /)
  get [path]             perform and HTTP GET request for path (default is /)
  create-index           create an index
```

```
~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu url
http://localhost:9200/
```

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu url 'some/path' -p 8080 -o mymachine.ull.es
http://mymachine.ull.es:8080/some/path
```

### Testing web services with json-server and Postman

* [json-server](https://github.com/typicode/json-server)
* [Video Create a Fake REST API with JSON-Server](https://youtu.be/1zkgdLZEdwM) Usa postman para los requests
* [Video en egg-head describiendo el uso de json-server](https://egghead.io/lessons/javascript-creating-demo-apis-with-json-server)
* [postman](https://www.getpostman.com/)
* [Video Create A REST API With JSON Server](https://youtu.be/x3NAo8zqdmo)

### Using request to Fetch JSON over HTTP

* [request npm](https://www.npmjs.com/package/request)
* [Código de `get path`](https://github.com/ULL-MII-CA-1819/nodejs-the-right-way/blob/master/commanding-databases-chapter-6/esclu/index.js#L46-L55)

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu -o api.github.com -p 80 -j get /repos/stedolan/jq/commits?per_page=5 | jq '.[0] | {message: .commit.message, name: .commit.committer.name}'
{
  "message": "Minor website fixes for 1.6",
  "name": "William Langford"
}
```

#### Creating an Elastic Index

* [Code of create-index](https://github.com/ULL-MII-CA-1819/nodejs-the-right-way/blob/master/commanding-databases-chapter-6/esclu/index.js#L63-L80)
* **Why PUT and no POST?**
    * [PUT vs. POST in REST](https://stackoverflow.com/questions/630453/put-vs-post-in-rest) Stack Overflow
    * Better is to choose between `PUT` and `POST` based on idempotence of the action.
    * `PUT` implies putting a resource - completely replacing whatever is available at the given URL with a different thing. By definition, a `PUT` is idempotent. Do it as many times as you like, and the result is the same. `x=5` is idempotent. You can `PUT` a resource whether it previously exists, or not (eg, to Create, or to Update)!
    * `POST` updates a resource, adds a subsidiary resource, or causes a change. A `POST` is not idempotent, in the way that `x++` is not idempotent.
    * By this argument, `PUT` is for creating when you know the URL of the thing you will create. `POST` can be used to create when you know the URL of the "factory" or manager for the category of things you want to create.
    * so: `POST /expense-report`
    * or: `PUT  /expense-report/10929`

#### Listing Elasticsearch indices

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu li -h
Usage: list-indices|li [options]

get a list of indices in this cluster

Options:
  -h, --help  output usage information

[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu li
health status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   prueba   HdtW7qEZTP-2Wx78ICygEA   5   1          0            0      1.2kb          1.2kb
yellow open   accounts 9TNc0k0LQ1e4y97yFX8_vg   5   1          2            0     10.4kb         10.4kb
yellow open   books    wP3DgQPZQZq0qBtH_dd0LA   5   1          0            0      1.2kb          1.2kb

[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu -j li | jq
{
  "accounts": {
    "aliases": {},
    "mappings": {
      "person": {
        "properties": {
          "job_description": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          },
          "lastname": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          },
          "name": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          }
        }
      }
    },
    "settings": {
      "index": {
        "creation_date": "1541679750244",
        "number_of_shards": "5",
        "number_of_replicas": "1",
        "uuid": "9TNc0k0LQ1e4y97yFX8_vg",
        "version": {
          "created": "6040299"
        },
        "provided_name": "accounts"
      }
    }
  },
  "books": {
    "aliases": {},
    "mappings": {},
    "settings": {
      "index": {
        "creation_date": "1542216727600",
        "number_of_shards": "5",
        "number_of_replicas": "1",
        "uuid": "wP3DgQPZQZq0qBtH_dd0LA",
        "version": {
          "created": "6040299"
        },
        "provided_name": "books"
      }
    }
  },
  "prueba": {
    "aliases": {},
    "mappings": {},
    "settings": {
      "index": {
        "creation_date": "1542217420653",
        "number_of_shards": "5",
        "number_of_replicas": "1",
        "uuid": "HdtW7qEZTP-2Wx78ICygEA",
        "version": {
          "created": "6040299"
        },
        "provided_name": "prueba"
      }
    }
  }
}
```

### Shaping JSON with jq

* [jq manual](https://stedolan.github.io/jq/manual/)
* [JSON on the command line with jq](https://shapeshed.com/jq-json/)
* YouTube vídeo [JSON: Like a Boss](https://youtu.be/_ZTibHotSew). Bob Tiernay explores the fascinating world of jq, "the JSON Processor”. Starting with a motivation, he then covers the language, provides helpful tips, showcases a real world example, cautions some things to avoid and finishes with a discussion of the ecosystem.
* [jq recipes](https://remysharp.com/drafts/jq-recipes)

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ jq -V
jq-1.5
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ jq --help
```
```
jq - commandline JSON processor [version 1.5]
Usage: jq [options] <jq filter> [file...]

	jq is a tool for processing JSON inputs, applying the
	given filter to its JSON text inputs and producing the
	filter's results as JSON on standard output.
	The simplest filter is ., which is the identity filter,
	copying jq's input to its output unmodified (except for
	formatting).
	For more advanced filters see the jq(1) manpage ("man jq")
	and/or https://stedolan.github.io/jq

	Some of the options include:
	 -c		compact instead of pretty-printed output;
	 -n		use `null` as the single input value;
	 -e		set the exit status code based on the output;
	 -s		read (slurp) all inputs into an array; apply filter to it;
	 -r		output raw strings, not JSON texts;
	 -R		read raw strings, not JSON texts;
	 -C		colorize JSON;
	 -M		monochrome (don't colorize JSON);
	 -S		sort keys of objects on output;
	 --tab	use tabs for indentation;
	 --arg a v	set variable $a to value <v>;
	 --argjson a v	set variable $a to JSON value <v>;
	 --slurpfile a f	set variable $a to an array of JSON texts read from <f>;
	See the manpage for more options.
```

See also the man pages:

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ man jq
```

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu li -j | jq '.books | keys'
[
  "aliases",
  "mappings",
  "settings"
]
```

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu get _stats | jq keys
[
  "_all",
  "_shards",
  "indices"
]
```

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu get _stats | jq -r 'path(..)| map(tostring) |join(".")' | head -n 20

_shards
_shards.total
_shards.successful
_shards.failed
_all
_all.primaries
_all.primaries.docs
_all.primaries.docs.count
_all.primaries.docs.deleted
_all.primaries.store
_all.primaries.store.size_in_bytes
_all.primaries.indexing
_all.primaries.indexing.index_total
_all.primaries.indexing.index_time_in_millis
_all.primaries.indexing.index_current
_all.primaries.indexing.index_failed
_all.primaries.indexing.delete_total
_all.primaries.indexing.delete_time_in_millis
_all.primaries.indexing.delete_current
```

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu get _stats | jq '._all.primaries | { count: .docs.count, size: .store.size_in_bytes}'
{
  "count": 58161,
  "size": 24424474
}
```

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ <  ../../transforming-data-and-testing-continuously-chapter-5/data/bulk_result.json jq .items[0,1,2]
```

### Inserting Elasticsearch Documents in Bulk

Recall that in ​chapter *Processing Data Files Sequentially​*, we developed an LDJ data file containing interleaved commands and documents for Elasticsearch’s bulk API:

```
[~/local/src/CA/sol-nodejs-the-right-way(master)]$ head transforming-data-and-testing-continuously-chapter-5/data/bulk_pg.ldj
{"index":{"_id":"pg1"}}
{"id":1,"title":"The Declaration of Independence of the United States of America","authors":["Jefferson, Thomas"],"subjects":["United States -- History -- Revolution, 1775-1783 -- Sources","United States. Declaration of Independence"]}
{"index":{"_id":"pg10"}}
{"id":10,"title":"The King James Version of the Bible","authors":[],"subjects":["Bible"]}
{"index":{"_id":"pg100"}}
{"id":100,"title":"The Complete Works of William Shakespeare","authors":["Shakespeare, William"],"subjects":["English drama -- Early modern and Elizabethan, 1500-1600"]}
{"index":{"_id":"pg1000"}}
{"id":1000,"title":"La Divina Commedia di Dante: Complete","authors":["Dante Alighieri"],"subjects":[]}
{"index":{"_id":"pg10000"}}
{"id":10000,"title":"The Magna Carta","authors":["Anonymous"],"subjects":["Constitutional history -- England -- Sources","Magna Carta"]}
```

* Code for command [`bulk <file>`](https://github.com/ULL-MII-CA-1819/nodejs-the-right-way/blob/master/commanding-databases-chapter-6/esclu/index.js#L119-L146)

Unlike the `get` and `url` commands that took an optional parameter, the `bulk` command’s ``<file>`` parameter is **required**:

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu bulk -h
Usage: bulk [options] <file>

read and perform bulk operations from the specified file

Options:
  -h, --help  output usage information
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu bulk
error: missing required argument `file'
```

Inside the action callback, the first thing we do is use [fs.stat](https://nodejs.org/api/fs.html#fs_fs_stat_path_options_callback) to asynchronously check on the provided file. This asserts that the file exists and can be reached by the user running the process.

Using the [size information from the stat call](https://nodejs.org/api/fs.html#fs_class_fs_stats), we can specify the HTTP header `content-length`.

This is important because we’ll be streaming the file content to the server rather than handing all the content at once.

Using `request.post`, we initialize an **HTTP POST request** to Elasticsearch, capturing the returned object in a variable called `req`.

```js
    const req = request.post(options);
```

This object can be used as a [writable stream (stream.Writable)](https://nodejs.org/dist/latest-v8.x/docs/api/stream.html#stream_writable_streams) for sending content,

We open a read stream to the file using [fs.createReadStream](https://nodejs.org/dist/latest-v8.x/docs/api/fs.html#fs_fs_createreadstream_path_options) and pipe that into the `req`uest object.

```js
const stream = fs.createReadStream(file);
stream.pipe(req);
```
and also as a readable stream (stream.Readable) for receiving the server’s response.
```js
      req.pipe(process.stdout);
```
The task `c6-build-books` in the `gulpfile.js` performs the bulk insertion:

```js
gulp.task("c6-build-books", shell.task(
  `commanding-databases-chapter-6/esclu/esclu bulk `+
  `transforming-data-and-testing-continuously-chapter-5/data/bulk_pg.ldj `+
  `-i books -t book > transforming-data-and-testing-continuously-chapter-5/data/bulk_result.json`
));
```

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ jq keys  ../../transforming-data-and-testing-continuously-chapter-5/data/bulk_result.json
[
  "errors",
  "items",
  "took"
]
```

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ jq '.items | length '  ../../transforming-data-and-testing-continuously-chapter-5/data/bulk_result.json
58161
```

### Implementing an Elasticsearch Query Command

 First we’ll take a look around using the existing `get` command, and then we’ll implement a specific command just for querying.

#### Finding the Schema of a JSON FILE with jq

Let us write a `jq` module with a function `schemas`:

```
 [~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ cat ~/.jq
def schemas: path(..) | map(tostring) | join(".");
```

It is saved in `~/.jq`

And use it:

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu get _search | jq -r 'schemas' | head -n 20

took
timed_out
_shards
_shards.total
_shards.successful
_shards.skipped
_shards.failed
hits
hits.total
hits.max_score
hits.hits
hits.hits.0
hits.hits.0._index
hits.hits.0._type
hits.hits.0._id
hits.hits.0._score
hits.hits.0._source
hits.hits.0._source.id
hits.hits.0._source.title
```

#### The result of a Search

1. We see a `took` field, which indicates how long the request took to execute in milliseconds.
2. The results of the query are in the `hits` object, which contains three fields:
    1. `hits.total`
    2. `hits.max_score`
        * The `max_score` field indicates the score value of the highest-scoring match.
        * Lucene (and thus Elasticsearch) uses the Boolean model to find matching documents, and
        * a formula called the **practical scoring function** to calculate relevance.
        * This formula borrows concepts from
            - term frequency/inverse document frequency and
            - the vector space model but
        * adds more-modern features like
            - a coordination factor,
            - field length   
            - normalization, and
            - term or query clause boosting.
        * See [scoring theory](https://www.elastic.co/guide/en/elasticsearch/guide/current/scoring-theory.html)
    3. `hits.hits`
        * The hits key points to an array of individual results.


##### By default the `_search` API will return only the top 10 results

Note that by default the `_search` API will return only the top 10 results. This can be increased by specifying the `size` URL parameter.

##### Searching for all books
```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu get _search | jq '[.hits.hits[]._source][0,1]'
{
  "id": 100,
  "title": "The Complete Works of William Shakespeare",
  "authors": [
    "Shakespeare, William"
  ],
  "subjects": [
    "English drama -- Early modern and Elizabethan, 1500-1600"
  ]
}
{
  "id": 1000,
  "title": "La Divina Commedia di Dante: Complete",
  "authors": [
    "Dante Alighieri"
  ],
  "subjects": []
}
```

##### Queries with `q`

Through the `_search` API, if you pass a query parameter, `q`,
Elasticsearch will use its value to find documents.

Say we were interested in books by Mark Twain. We could search for documents whose authors array includes the substring `Twain` using the query expression `q="authors:Twain"`, like this:

```
[~/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu get _search?q="authors:Twain" | jq '.hits.hits[0]'
{
  "_index": "books",
  "_type": "book",
  "_id": "pg1837",
  "_score": 6.9273376,
  "_source": {
    "id": 1837,
    "title": "The Prince and the Pauper",
    "authors": [
      "Twain, Mark"
    ],
    "subjects": [
      "Edward VI, King of England, 1537-1553 -- Fiction",
      "Lookalikes -- Fiction",
      "London (England) -- Fiction",
      "Historical fiction",
      "Impostors and imposture -- Fiction",
      "Boys -- Fiction",
      "Princes -- Fiction",
      "Social classes -- Fiction",
      "Poor children -- Fiction"
    ]
  }
}

[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu get _search?q="authors:Twain" | jq '.hits.hits[] | ._source.title'
"The Prince and the Pauper"
"Chapters from My Autobiography"
"The Awful German Language"
"Personal Recollections of Joan of Arc — Volume 1"
"Personal Recollections of Joan of Arc — Volume 2"
"In Defence of Harriet Shelley"
"The Innocents Abroad"
"The Mysterious Stranger, and Other Stories"
"The Curious Republic of Gondour, and Other Whimsical Sketches"
"The Innocents Abroad — Volume 03"
```
##### Query Syntax Features

Elasticsearch’s query string syntax is a DSL with many useful features like

1. wildcards,
2. Boolean AND/OR operators,
3. negation, and
4. even regular expressions.

##### Source Filtering

Elasticsearch supports [source filtering](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-source-filtering.html).

We could use the source filter expression `_source=title`. It is a bit like ecma6 selecting only the field `title` of the corresponding object.

```
[~/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu get _search?q="authors:Twain&_source=title" | jq '.hits.hits[0]'
{
  "_index": "books",
  "_type": "book",
  "_id": "pg1837",
  "_score": 6.9273376,
  "_source": {
    "title": "The Prince and the Pauper"
  }
}
```
Now the `_source` objects contain only the `title` key.

#### Implementing an Elasticsearch Query Command: `esclu q authors:Twain`

1. The command will be called `query`, with the alias `q` for short.
2. It will take any number of optional query parts, so the user won’t have to wrap the query in quotes.
3. It can be specified an optional source filter expression to limit the output documents.

Example:

```
​ 	​$ ​​./esclu​​ ​​q​​ ​​authors:Twain​​ ​​AND​​ ​​subjects:children​
```

* [Solution](https://github.com/ULL-MII-CA-1819/nodejs-the-right-way/blob/master/commanding-databases-chapter-6/esclu/index.js#L149-L165)

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu q authors:Twain AND subjects:children | jq .hits.hits[0,1]
{
  "_index": "books",
  "_type": "book",
  "_id": "pg7195",
  "_score": 9.074243,
  "_source": {
    "id": 7195,
    "title": "The Adventures of Tom Sawyer, Part 3.",
    "authors": [
      "Twain, Mark"
    ],
    "subjects": [
      "Humorous stories",
      "Child witnesses -- Fiction",
      "Boys -- Fiction",
      "Sawyer, Tom (Fictitious character) -- Fiction",
      "Male friendship -- Fiction",
      "Mississippi River Valley -- Fiction",
      "Bildungsromans",
      "Adventure stories",
      "Missouri -- Fiction",
      "Runaway children -- Fiction"
    ]
  }
}
{
  "_index": "books",
  "_type": "book",
  "_id": "pg1837",
  "_score": 8.96923,
  "_source": {
    "id": 1837,
    "title": "The Prince and the Pauper",
    "authors": [
      "Twain, Mark"
    ],
    "subjects": [
      "Edward VI, King of England, 1537-1553 -- Fiction",
      "Lookalikes -- Fiction",
      "London (England) -- Fiction",
      "Historical fiction",
      "Impostors and imposture -- Fiction",
      "Boys -- Fiction",
      "Princes -- Fiction",
      "Social classes -- Fiction",
      "Poor children -- Fiction"
    ]
  }
}
```

```
~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu q authors:Shakespeare AND subjects:drama -f title,authors,subjects | jq '.hits.hits | first, last | ._source'
{
  "subjects": [
    "Spirits -- Drama",
    "Tragicomedy",
    "Political refugees -- Drama",
    "Magicians -- Drama",
    "Shipwreck victims -- Drama",
    "Islands -- Drama",
    "Fathers and daughters -- Drama"
  ],
  "title": "The Tempest",
  "authors": [
    "Shakespeare, William"
  ]
}
{
  "subjects": [
    "Tragedies",
    "Denmark -- Drama",
    "Hamlet (Legendary character) -- Drama",
    "Princes -- Drama",
    "Revenge -- Drama",
    "Kings and rulers -- Succession -- Drama",
    "Fathers -- Death -- Drama",
    "Murder victims' families -- Drama"
  ],
  "title": "Hamlet",
  "authors": [
    "Shakespeare, William"
  ]
}
```

### Challenge: Deleting an Index

Any database you work with will offer at least the following four CRUD operations:
1. Create,
2. Read,
3. Update, and
4. Delete

RESTful datastores like Elasticsearch use a different HTTP method (or verb) for each operation:
1. You use POST to create,
2. GET to read,
3. PUT to update, and
4. DELETE to delete

Implement a new command called `delete-index`,
which checks for an index specified with the `--index`
flag and issues an `HTTP DELETE` request to remove it.

### Challenge: Adding a Single Document

Add a new command called `put`, which inserts a new document for indexing (or overwrites the existing document if there’s a collision).

With the `get` command, you can already retrieve a book by its `_id`.
For example, here’s how to look up `The Art of War` by its ID:

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6/esclu(master)]$ ./esclu  get pg132 --index books --type book | jq .
{
  "_index": "books",
  "_type": "book",
  "_id": "pg132",
  "_version": 1,
  "found": true,
  "_source": {
    "id": 132,
    "title": "The Art of War",
    "authors": [
      "Sunzi, active 6th century B.C.",
      "Giles, Lionel"
    ],
    "subjects": [
      "Military art and science -- Early works to 1800",
      "War -- Early works to 1800"
    ]
  }
}
```

For example, say we save the document part of the above response to a file, like so:

```
$ ./esclu  get pg132 --index books --type book | jq '._source' > ../data/art_of_war.json
```

Ideally, we should be able to reinsert the document from the file using the following command:
```
$ ./esclu  put ../data/art_of_war.json -i books -t book --id pg132
```

To make this work, you’ll need to do the following:

1. Add a new, optional, `--id` flag.
2. Update the `fullUrl` function to append the `ID` in the returned URL.
3. Add a new command called `put` that takes a single required parameter `file`
4. Inside the action callback of your new command, assert that an `ID` was specified, or fail loudly.
5. Stream the contents of the file to Elasticsearch through the `request` object and stream the results to standard output.

### References

* [A Practical Introduction to Elasticsearch
By Ismael Hasan Romero](https://www.elastic.co/blog/a-practical-introduction-to-elasticsearch)
* [Build a Search Engine with Node.js and Elasticsearch
September 27, 2016, By Behrooz Kamali](https://www.sitepoint.com/search-engine-node-elasticsearch/) SitePoint
* [Elasticsearch & Node.js Getting Started](https://medium.com/@siddharthac6/elasticsearch-node-js-b16ea8bec427)

* [http://lucene.apache.org/](http://lucene.apache.org/)

* [http://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html](http://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html)

* [https://www.elastic.co/downloads/past-releases/elasticsearch-5-2-2](https://www.elastic.co/downloads/past-releases/elasticsearch-5-2-2)

* [https://www.elastic.co/guide/en/elasticsearch/reference/5.2/system-config.html](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/system-config.html)

* [https://nodejs.org/api/http.html](https://nodejs.org/api/http.html)

* [https://www.npmjs.com/package/yargs](https://www.npmjs.com/package/yargs)

* [https://www.npmjs.com/package/superagent](https://www.npmjs.com/package/superagent)

* [https://www.npmjs.com/package/node-fetch](https://www.npmjs.com/package/node-fetch)

* [https://developer.mozilla.org/en-US/docs/Web/API/WindowOrWorkerGlobalScope/fetch](https://developer.mozilla.org/en-US/docs/Web/API/WindowOrWorkerGlobalScope/fetch)

* [https://stedolan.github.io/jq/](https://stedolan.github.io/jq/)

* [https://stedolan.github.io/jq/manual/v1.5/#Builtinoperatorsandfunctions](https://stedolan.github.io/jq/manual/v1.5/#Builtinoperatorsandfunctions)

* [https://stedolan.github.io/jq/manual/v1.5/#Basicfilters](https://stedolan.github.io/jq/manual/v1.5/#Basicfilters)

* [https://stedolan.github.io/jq/manual/v1.5/](https://stedolan.github.io/jq/manual/v1.5/)

* [https://nodejs.org/api/stream.html](https://nodejs.org/api/stream.html)

* [https://www.elastic.co/guide/en/elasticsearch/reference/5.2/query-dsl-query-string-query.html#query-string-syntax](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/query-dsl-query-string-query.html#query-string-syntax)

* [https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-source-filtering.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-source-filtering.html)

* [https://www.elastic.co/guide/en/elasticsearch/reference/5.2/docs-index_.html](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/docs-index_.html)
