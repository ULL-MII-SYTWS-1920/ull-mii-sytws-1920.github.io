---
layout: post
title:  "Clase del Lunes 16/12/2019"
categories: Clases
---

# Clase del Lunes 16/12/2019

## Recapping the *Transforming Data and Testing Continuously* Lesson

### [test/parse-rdf-test.js]({{page.code}}/test/parse-rdf-test.js)

```js
​ 'use strict';

const fs = require('fs');
const expect = require('chai').expect;
const parseRDF = require('../lib/parse-rdf.js');
const rdf = fs.readFileSync(`${__dirname}/pg132.rdf`);

describe("Chapter 5: Transforming Data and Testing Continuously", () => {
  describe('parseRDF', () => {
    it('should be a function', () => {
      debugger;
      expect(parseRDF).to.be.a('function');
    });
    it('should parse RDF content', () => {
      const book = parseRDF(rdf);
      // https://www.chaijs.com/api/bdd/#method_language-chains
      expect(book).to.be.a('object');

      expect(book).to.have.a.property('id', 132);

      expect(book).to.have.a.property('title', 'The Art of War');

      expect(book).to.have.a.property('authors').that.is.an('array').with.lengthOf(2).
      and.contains('Sunzi, active 6th century B.C.').
      and.contains('Giles, Lionel');

      expect(book).to.have.a.property('subjects').that.is.an('array').with.lengthOf(2).
      and.contains('Military art and science -- Early works to 1800').
      and.contains('War -- Early works to 1800');
    });
  });
});
```

### [lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js)

```js
'use strict';
const cheerio = require("cheerio");

module.exports = rdf => {
  const $ = cheerio.load(rdf);
  const book = {};

  /* We're trying to grab the number 132  out of this XML tag:
    <pgterms:ebook rdf:about="ebooks/132">
  */
  book.id = +  // Convert to a number
     $('pgterms\\:ebook') // In CSS, the colon : is used for pseudo selectors (like :hover) that is why we have to escape it
    .attr('rdf:about') // get the rdf:about attribute
    .replace('ebooks/','');
  // Find   <dcterms:title>The Art of War</dcterms:title>
  book.title = $('dcterms\\:title').text();

  book.authors = $('pgterms\\:agent pgterms\\:name')
                 .toArray() // The collection object returned is not a true Array we have to convert it
                 .map(
                   (e)  => { // e is a document node has no text method
                             // that is why we wrap it $(e) to a cheerio object
                     return $(e).text();
                   }
                 );


  book.subjects = $('[rdf\\:resource$="/LCSH"]').
                  parent() // https://api.jquery.com/parent/
                  .find('rdf\\:value') // https://api.jquery.com/find/
                  .toArray()
                  .map(e => $(e).text());
  return book;
};
```

Using this, we can now quickly put together a command-line program to explore some of the other RDF files. Open your editor and enter this:

### [rdf-to-json.js]({{page.code}}/rdf-to-json.js)

```js
#!/usr/bin/env node
const fs = require('fs');
const parseRDF = require('./lib/parse-rdf.js');
const rdf = fs.readFileSync(process.argv[2]);
const book = parseRDF(rdf);
console.log(JSON.stringify(book, null, '  '));
```

When calling [`JSON.stringify`](https://developer.mozilla.org/es/docs/Web/JavaScript/Referencia/Objetos_globales/JSON/stringify) we’re passing three arguments: 
1. The second argument (`null`) is an optional replacer function that can be used for filtering
2. The last argument (`' '`) is used to indent nested objects, making the output more human-readable.

Let’s open a terminal in our databases project directory and run it:

```
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5(master)]$ cd databases/
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ls -l rdf-to-json.js 
-rw-r--r--  1 casiano  staff  217  7 nov 13:36 rdf-to-json.js
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ chmod a+x rdf-to-json.js 
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ls -l rdf-to-json.js 
-rwxr-xr-x  1 casiano  staff  217  7 nov 13:36 rdf-to-json.js
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ ./rdf-to-json.js ../data/cache/epub//11/pg11.rdf 
{
  "id": 11,
  "title": "Alice's Adventures in Wonderland",
  "authors": [
    "Carroll, Lewis"
  ],
  "subjects": [
    "Fantasy literature"
  ]
}
```


### Processing Data Files Sequentially

Our [lib/parse-rdf.js]({{page.code}}/lib/parse-rdf.js)  converts RDF content into JSON documents. 

All that remains is to walk through the Project Gutenberg catalog directory and collect all the JSON documents.

More concretely, we need to do the following:

1. Traverse down the `data/cache/epub` directory looking for files ending in `rdf`.
2. Read each RDF file.
3. Run the RDF content through `parseRDF`.
4. Collect the JSON serialized objects into a single, bulk file for insertion.


The NoSQL database we’ll be using is Elasticsearch, a document datastore that indexes JSON objects.

**GOAL**: We want to transform the Gutenberg data into an intermediate form for bulk import.

[Elasticsearch has a bulk-import API that lets you pull in many records at once](https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/api-reference.html)

Although we could insert them one at a time, it is significantly faster to use the bulk-insert API.

[The format of the file we need to create is described on Elasticsearch’s Bulk API page](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html)

It’s an LDJ file consisting of 
1. **actions** and 
2. the **source objects on which to perform each action**.

In our case, we’re performing `index` operations—that is, 
**inserting new documents into an index**.

Each source object is the book object returned by `parseRDF`.
Here’s an example of an action followed by its source object:

```
​  {​"index"​:{​"_id"​:​"pg11"​}}
​  {​"id"​:11,​"title"​:​"Alice's Adventures in Wonderland"​,​"authors"​:...}
```

And here’s another one:

```
​   {​"index"​:{​"_id"​:​"pg132"​}}
​   {​"id"​:132,​"title"​:​"The Art of War"​,​"authors"​:...}
```

In each case,
1. an action is a JSON object on a line by itself, and 
2. the source object is another JSON object on the next line.

Elasticsearch’s bulk API allows you to chain any number of these together like so:

```
  {​"index"​:{​"_id"​:​"pg11"​}}
​  {​"id"​:11,​"title"​:​"Alice's Adventures in Wonderland"​,​"authors"​:...}
​  {​"index"​:{​"_id"​:​"pg132"​}}
​  {​"id"​:132,​"title"​:​"The Art of War"​,​"authors"​:...}
```


To find and open each of the `RDF` files under the `data/cache/epub` directory, we will use a module called [node-dir](https://github.com/fshost/node-dir).

Install it:

```
​​$ ​​npm​​ ​​install​​ ​​​​node-dir
```

This module comes with a handful of useful methods for walking a directory tree. 

The method we’ll use is 
`readFiles`, 
which sequentially operates on files as it encounters them while walking a directory tree.
<details>
 <summary>
 <b>Argumentos de readFiles</b>
 </summary>
  <ul>
    <li>encoding: file encoding (defaults to 'utf8')</li>
    <li>exclude: a regex pattern or array to specify filenames to ignore</li>
    <li>excludeDir: a regex pattern or array to specify directories to ignore</li>
    <li>match: a regex pattern or array to specify filenames to operate on</li>
    <li>matchDir: a regex pattern or array to specify directories to recurse</li>
    <li>recursive: whether to recurse subdirectories when reading files (defaults to true)</li>
    <li>reverse: sort files in each directory in descending order</li>
    <li>shortName: whether to aggregate only the base filename rather than the full filepath</li>
    <li>sort: sort files in each directory in ascending order (defaults to true)</li>
    <li>doneOnErr: control if done function called on error (defaults to true)</li>
  </ul>
</details>


**[rdf-to-bulk]({{page.code}}/rdf-to-bulk.js)**

```js
// Run as:
//          node rdf-to-bulk.js ../data/cache/epub | head
// or       gulp c5-rdf-to-json-11
'use strict';
const dir = require('node-dir');
const parseRDF = require(__dirname+'/lib/parse-rdf.js');
const dirname = process.argv[2];

const options = {
  match: /\.rdf$/,
  exclude: ['pg0.rdf']
};

// Because the head command closes the pipe after echoing the beginning lines
// we capture error events on the process.stdout stream
process.stdout.on('error', err => process.exit());

dir.readFiles(dirname, options, (err, content, next) => {
  if (err) throw err;
  const doc = parseRDF(content);
  console.log(JSON.stringify({ index: { _id: `pg${doc.id}`} }));
  console.log(JSON.stringify(doc));
  next();
});

```

The `_id` field of each index operation is the unique identifier that Elasticsearch will use for the document.

Here the author has chosen to use the string `pg` followed by the Project Gutenberg ID. This way, if we ever wanted to store documents from another source in the same index, they shouldn’t collide with the Project Gutenberg book data.

Run the program like that:

```
~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ node rdf-to-bulk.js ../data/cache/epub | head
{"index":{"_id":"pg1"}}
{"id":1,"title":"The Declaration of Independence of the United States of America","authors":["Jefferson, Thomas"],"subjects":["United States -- History -- Revolution, 1775-1783 -- Sources","United States. Declaration of Independence"]}
{"index":{"_id":"pg10"}}
{"id":10,"title":"The King James Version of the Bible","authors":[],"subjects":["Bible"]}
{"index":{"_id":"pg100"}}
{"id":100,"title":"The Complete Works of William Shakespeare","authors":["Shakespeare, William"],"subjects":["English drama -- Early modern and Elizabethan, 1500-1600"]}
{"index":{"_id":"pg1000"}}
{"id":1000,"title":"La Divina Commedia di Dante: Complete","authors":["Dante Alighieri"],"subjects":[]}
{"index":{"_id":"pg10000"}}
{"id":10000,"title":"The Magna Carta","authors":["Anonymous"],"subjects":["Constitutional history -- England -- Sources","Magna Carta"]}
[~/local/src/CA/sol-nodejs-the-right-way/transforming-data-and-testing-continuously-chapter-5/databases(master)]$ 
```

Because the head command closes the pipe after echoing the beginning lines, this can sometimes cause Node.js to throw an exception.
We can capture error events on the `process.stdout` stream. That is why we have added  the following line to `rdf-to-bulk.js`:

```js
​ 	process.stdout.on(​'error'​, err => process.exit());
```

Use the following command to capture the output in a new file called `bulk_pg.ldj`.
```
 ​​node​​ ​​rdf-to-bulk.js​​ ​​../data/cache/epub/​​ ​​>​​ ​​../data/bulk_pg.ldj​
```

Esto lleva su tiempo, en mi máquina ocupa mas de 12MB:

```
$ ls -lh data/
total 46664
-rw-r--r--  1 casiano  staff   294B 29 oct  2018 README.md
-rw-r--r--  1 casiano  staff    12M 11 mar  2019 bulk_pg.ldj
-rw-r--r--  1 casiano  staff    10M 20 nov  2018 bulk_result.json
drwxr-xr-x  3 casiano  staff    96B 25 oct  2018 cache
```

### Descripción de la práctica p9-t3-transforming-data: Extracting Classification Codes and Sources

* [La práctica p9-t3-transforming-data](/tema3-web/practicas/p9-t3-transforming-data/)

## El Reto: Precios de Hiperdino

* [Escriba un programa que liste los precios de Hiperdino por categoría del producto]({{site.url}}/tema3-web/practicas/p9-t3-transforming-data/reto)

## Chapter 6: Commanding DataBases

En este capítulo construiremos un programa de línea de comandos `esclu` 
para interactuar con la base de datos Elasticsearch.

Elasticsearch is a schema-free, RESTful, NoSQL database that stores and indexes JSON documents over HTTP.

Nuestro programa `esclu` hará requests REST al servidor de ElasticSearch y este retornará un JSON con la respuesta al 
request. Sigue un ejemplo de ejecución. 

Puesto que la salida es un JSON muy grande, la hemos canalizado a un programa 
[`jq`](https://stedolan.github.io/jq/tutorial/) (JSON query language):


```
$ ./esclu query authors:Twain AND subjects:children | jq .hits.hits[1]._source
{
  "id": 1837,
  "title": "The Prince and the Pauper",
  "authors": [
    "Twain, Mark"
  ],
  "subjects": [
    "Edward VI, King of England, 1537-1553 -- Fiction",
    "Lookalikes -- Fiction",
    "London (England) -- Fiction",
    "Historical fiction",
    "Impostors and imposture -- Fiction",
    "Boys -- Fiction",
    "Princes -- Fiction",
    "Social classes -- Fiction",
    "Poor children -- Fiction"
  ]
}
```

Lo que nos permite el DSL `jq` es navegar a través del JSON y obtener el campo en el que estamos interesados. En
el ejemplo anterior la salida del comando `./esclu query authors:Twain AND subjects:children` es un JSON  como este:

```
{
  "took": 37,
  ...
  "hits": {
    "total": 41,
    "max_score": 9.074243,
    "hits": [
      { ... },
      {
        ...
        "_id": "pg7195",
        "_source": {
            "id": 1837,
            "title": "The Prince and the Pauper",
          ...
        }
      },
      ...
  }
```

De manera que con la expresión  `jq .hits.hits[1]._source` describimos un elemento concreto del JSON.

[Elasticsearch](https://es.wikipedia.org/wiki/Elasticsearch) es un servidor de búsqueda basado en Lucene. Provee un motor de búsqueda de texto completo, distribuido y con [capacidad de multi-tenencia](https://es.wikipedia.org/wiki/Tenencia_m%C3%BAltiple) con una interfaz web RESTful y con documentos JSON. Elasticsearch está desarrollado en Java y está publicado como código abierto bajo las condiciones de la licencia Apache.

Usando el comando `esclu bulk` cargaremos los documentos JSON generados en el capítulo
anterior en la base de datos de Elasticsearch:

```
$ ./esclu bulk ../data/bulk_pg.ldj -i books -t book > ../data/bulk_result.json
```


![/assets/images/ch6-xml-2-json-ch6-2-es.png](/assets/images/ch6-xml-2-json-ch6-2-es.png)




### Version

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6(master)]$ java --version
java 9.0.4
Java(TM) SE Runtime Environment (build 9.0.4+11)
Java HotSpot(TM) 64-Bit Server VM (build 9.0.4+11, mixed mode)
```

```
[~/local/src/CA/sol-nodejs-the-right-way/commanding-databases-chapter-6(master)]$ elasticsearch --version
Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
Version: 6.4.2, Build: default/tar/04711c2/2018-09-26T13:34:09.098244Z, JVM: 9.0.4
```

### Mi instalación de ElasticSearch en el MacAir
```
[~/campus-virtual/1819/ca1819/practicas(master)]$ cat ~/.bash_profile | sed -ne '/elastic/,/^$/p'
# Make source "~/bin/elasticsearch-set"
source ~/bin/elasticsearch-set
```

```
[~/campus-virtual/1819/ca1819/practicas(master)]$ cat ~/bin/elastic-search-set
# ElasticSearch
export ES_HOME=~/Applications/elasticsearch-6.4.2
# export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_77/Contents/Home
#export PATH=$ES_HOME/bin:$JAVA_HOME/bin:$PATH
export PATH=$ES_HOME/bin:$PATH
```

### Queries

Elasticsearch utiliza Query DSL (Lenguaje de dominio específico) para realizar las consultas a los documentos indexados. Es un lenguaje sumamente flexible y de gran alcance, además de simple, que permite conocer y explorar los datos de la mejor manera. Al ser utilizado a través de una interfaz de tipo JSON, las consultas son muy sencillas de leer y, lo más importante, de depurar.

### Clusters

* [Creating an Elasticsearch Cluster: Getting Started](https://logz.io/blog/elasticsearch-cluster-tutorial/)
* [Shard](https://en.wikipedia.org/wiki/Shard_(database_architecture))
    - A database shard is a horizontal partition of data in a database or search engine. Each individual partition is referred to as a shard or database shard. Each shard is held on a separate database server instance, to spread load.
    - Some data within a database remains present in all shards but some appears only in a single shard. Each shard (or server) acts as the single source for this subset of data
    - ![sharding in mongodb](https://docs.mongodb.com/v3.0/_images/sharded-collection.png)
    -  [¿Cómo se reparten? Usando las llamadas "*shard keys*" o *claves de repartición*. Cada partición contiene un intervalo de claves (Clave Mínima, Clave Máxima). Se habla de "*partición basada en rangos*". La *Index Big Table* de Google utiliza una idea similar. ](http://gpd.sip.ucm.es/rafa/docencia/nosql/Sharding.html)
    - [Tutorial MongoDB. Explicando el sharding con una baraja de cartas](https://charlascylon.com/2014-01-30-tutorial-mongodb-explicando-el-sharding-con-una)


### Running ElasticSearch

```
[~]$ which elasticsearch
/Users/casiano/Applications/elasticsearch-6.4.2/bin/elasticsearch
[~]$ elasticsearch
Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2018-11-14T10:37:38,855][INFO ][o.e.n.Node               ] [] initializing ...
[2018-11-14T10:37:39,061][INFO ][o.e.e.NodeEnvironment    ] [9jAGWs_] using [1] data paths, mounts [[/ (/dev/disk1s1)]], net usable_space [54.1gb], net total_space [233.5gb], types [apfs]
[2018-11-14T10:37:39,062][INFO ][o.e.e.NodeEnvironment    ] [9jAGWs_] heap size [990.7mb], compressed ordinary object pointers [true]
[2018-11-14T10:37:39,124][INFO ][o.e.n.Node               ] [9jAGWs_] node name derived from node ID [9jAGWs_uQGmUPF4RyFkjTw]; set [node.name] to override
[2018-11-14T10:37:39,125][INFO ][o.e.n.Node               ] [9jAGWs_] version[6.4.2], pid[18289], build[default/tar/04711c2/2018-09-26T13:34:09.098244Z], OS[Mac OS X/10.13.6/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/9.0.4/9.0.4+11]
[2018-11-14T10:37:39,127][INFO ][o.e.n.Node               ] [9jAGWs_] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/var/folders/5g/l7k4y5112612gk6qxzy6jwv40000gn/T/elasticsearch.R6oT3jtY, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Djava.locale.providers=COMPAT, -Des.path.home=/Users/casiano/Applications/elasticsearch-6.4.2, -Des.path.conf=/Users/casiano/Applications/elasticsearch-6.4.2/config, -Des.distribution.flavor=default, -Des.distribution.type=tar]
[2018-11-14T10:37:43,567][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [aggs-matrix-stats]
[2018-11-14T10:37:43,568][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [analysis-common]
[2018-11-14T10:37:43,568][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [ingest-common]
[2018-11-14T10:37:43,568][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [lang-expression]
[2018-11-14T10:37:43,569][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [lang-mustache]
[2018-11-14T10:37:43,569][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [lang-painless]
[2018-11-14T10:37:43,569][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [mapper-extras]
[2018-11-14T10:37:43,570][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [parent-join]
[2018-11-14T10:37:43,570][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [percolator]
[2018-11-14T10:37:43,570][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [rank-eval]
[2018-11-14T10:37:43,570][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [reindex]
[2018-11-14T10:37:43,571][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [repository-url]
[2018-11-14T10:37:43,571][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [transport-netty4]
[2018-11-14T10:37:43,571][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [tribe]
[2018-11-14T10:37:43,571][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-core]
[2018-11-14T10:37:43,572][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-deprecation]
[2018-11-14T10:37:43,572][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-graph]
[2018-11-14T10:37:43,572][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-logstash]
[2018-11-14T10:37:43,572][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-ml]
[2018-11-14T10:37:43,573][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-monitoring]
[2018-11-14T10:37:43,574][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-rollup]
[2018-11-14T10:37:43,574][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-security]
[2018-11-14T10:37:43,575][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-sql]
[2018-11-14T10:37:43,575][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-upgrade]
[2018-11-14T10:37:43,576][INFO ][o.e.p.PluginsService     ] [9jAGWs_] loaded module [x-pack-watcher]
[2018-11-14T10:37:43,577][INFO ][o.e.p.PluginsService     ] [9jAGWs_] no plugins loaded
[2018-11-14T10:37:51,783][INFO ][o.e.x.s.a.s.FileRolesStore] [9jAGWs_] parsed [0] roles from file [/Users/casiano/Applications/elasticsearch-6.4.2/config/roles.yml]
[2018-11-14T10:37:53,514][INFO ][o.e.x.m.j.p.l.CppLogMessageHandler] [controller/18309] [Main.cc@109] controller (64 bit): Version 6.4.2 (Build 660eefe6f2ea55) Copyright (c) 2018 Elasticsearch BV
[2018-11-14T10:37:54,999][DEBUG][o.e.a.ActionModule       ] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security
[2018-11-14T10:37:55,519][INFO ][o.e.d.DiscoveryModule    ] [9jAGWs_] using discovery type [zen]
[2018-11-14T10:37:57,255][INFO ][o.e.n.Node               ] [9jAGWs_] initialized
[2018-11-14T10:37:57,256][INFO ][o.e.n.Node               ] [9jAGWs_] starting ...
[2018-11-14T10:37:57,673][INFO ][o.e.t.TransportService   ] [9jAGWs_] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
[2018-11-14T10:38:00,854][INFO ][o.e.c.s.MasterService    ] [9jAGWs_] zen-disco-elected-as-master ([0] nodes joined)[, ], reason: new_master {9jAGWs_}{9jAGWs_uQGmUPF4RyFkjTw}{zMF8WZ_BTRuJ9fsz9AZpZQ}{127.0.0.1}{127.0.0.1:9300}{ml.machine_memory=8589934592, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true}
[2018-11-14T10:38:00,867][INFO ][o.e.c.s.ClusterApplierService] [9jAGWs_] new_master {9jAGWs_}{9jAGWs_uQGmUPF4RyFkjTw}{zMF8WZ_BTRuJ9fsz9AZpZQ}{127.0.0.1}{127.0.0.1:9300}{ml.machine_memory=8589934592, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true}, reason: apply cluster state (from master [master {9jAGWs_}{9jAGWs_uQGmUPF4RyFkjTw}{zMF8WZ_BTRuJ9fsz9AZpZQ}{127.0.0.1}{127.0.0.1:9300}{ml.machine_memory=8589934592, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true} committed version [1] source [zen-disco-elected-as-master ([0] nodes joined)[, ]]])
[2018-11-14T10:38:00,964][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [9jAGWs_] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}
[2018-11-14T10:38:00,966][INFO ][o.e.n.Node               ] [9jAGWs_] started
[2018-11-14T10:38:02,581][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [9jAGWs_] Failed to clear cache for realms [[]]
[2018-11-14T10:38:02,663][INFO ][o.e.l.LicenseService     ] [9jAGWs_] license [aee13e9e-1119-43f0-a026-62ba967c1c8c] mode [basic] - valid
[2018-11-14T10:38:02,698][INFO ][o.e.g.GatewayService     ] [9jAGWs_] recovered [2] indices into cluster_state
[2018-11-14T10:38:04,870][INFO ][o.e.c.r.a.AllocationService] [9jAGWs_] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[accounts][3]] ...]).
```

Notice the line:
```
[2018-11-14T10:37:57,673][INFO ][o.e.t.TransportService   ] [9jAGWs_] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
```

You can specify a lot of settings when setting up an Elasticsearch cluster. We haven’t specified any here, which means it’s running in development mode.

* [Elastic’s System Configuration page](https://www.elastic.co/guide/en/elasticsearch/reference/current/system-config.html)
